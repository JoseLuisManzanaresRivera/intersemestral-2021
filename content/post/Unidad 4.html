---
title: "Unidad 4"
output: html_document
date: '2022-12-14'
---

<script src="Unidad 4_files/header-attrs/header-attrs.js"></script>


<div id="regresión-lineal-simple." class="section level2">
<h2>Regresión Lineal simple.</h2>
<div id="objetivo-de-la-sesión" class="section level5">
<h5>Objetivo de la sesión:</h5>
<p>Presentar el modelo de Regresión lineal. Orígenes del concepto de regresión, características y estructura del modelo, expresión matemática.</p>
<p><em>All models are wrong, but some are useful. Box (1979)</em></p>
</div>
<div id="metas-de-la-sesión-de-hoy" class="section level5">
<h5>Metas de la sesión de hoy:</h5>
<ul>
<li><p>Aprender a Modelar relaciones estadísticas (estocásticas) no deterministas. Mediante el modelo de regresión lineal.</p></li>
<li><p>Conocer la técnica de mínimos cuadrados ordinarios.</p></li>
</ul>
</div>
<div id="orígenes-de-la-noción-de-regresión" class="section level3">
<h3>Orígenes de la noción de regresión</h3>
<p>La noción de Regresión lineal. <strong>¿Qué entendemos por regresión?</strong></p>
<p>A pesar de las multiples contribuciones y desarrollos recientes sobre la aplicación de este concepto en el contexto del análisis estadístico el desarrollo de la técnica se debe al francés <strong>Legendre en 1805</strong> y su uso generalizado bajo la denominación del modelo de <strong>regresión lineal</strong> se atribuye al antropólogo británico <strong>Francis Galton</strong> (primo de <strong>Charles Darwin</strong>)</p>
<p>De acuerdo con Stigler en su libro <a href="https://drive.google.com/file/d/1NgCTeIg4Tclt-TsKpHV_wmXvk4TY7dZ9/view?usp=sharing">Statistics on the table</a> <em>“The story is an exciting one involving, science, experiment, mathematics, simulation and one of the great thougth experiments of all times.”</em> (Stigler, 1999)</p>
<p>De acuerdo con <strong>Galton</strong> en su obra <a href="https://drive.google.com/file/d/1Zd3ORNina7jKPXe176nhUNP3wRSElsMv/view?usp=sharing">Hereditary Genius 1869</a>. La mezcla de características genéticas a lo largo de la descendencia explica el grado de talento observado en los individuos.</p>
<p>Su trabajo aborda entre otros elementos como el talento se manifiesta através de las generaciones. Ej. los Bachs, (Johann Sebastian Bach) los Bernoulli, (Daniel Bernoulli ), etc.</p>
<p>Sus observaciones parecian indicar que el talento sistemáticamente tendía a disminuir a medida que se consideraba a los miembros de la familia más distantes, tanto hacia “arriba” en el árbol genealógico como hacia “abajo”. La observación se manifiesta incluso al referirnos a otras especies.</p>
<p>Galton: <em>“if a man breeds from strong, well-shaped dogs, but of mixed pedigree, the puppies will be sometimes, but rarely, the equals of their parents. They will commonly be of a mongrel, nondescript type, because ancenstral peculiarities are apt to crop out in the offspring”.</em></p>
<div id="definiciones." class="section level4">
<h4>Definiciones.</h4>
<p>Regresión es el estudio de la distribucion condicional de <span class="math inline">\(Y|x\)</span> de la variable de respuesta <strong>Y</strong> dado un vector <span class="math inline">\(pX1\)</span> de variables predictivas. En el modelo de regresión lineal <span class="math display">\[Y=\beta^T X+\epsilon\]</span></p>
<p>Definición preliminar sobre el concepto de regresión.</p>
<p><span class="math inline">\(Y\)</span> es condicionalmente dependiente de <span class="math inline">\(x\)</span> dada una combinación lineal <span class="math inline">\(\beta^Tx\)</span> de las variables predictivas.</p>
<p>Básicamente estamos modelando una <strong>relación de dependencia</strong>. En este caso la variable dependiente es <strong>cuantitativa</strong>, no categórica, si bien los regresores o variables <strong>explicativas(independientes)</strong> pueden ser de tipo categórico o numérico, (considerando que se realiza la codificación apropiada).</p>
<p>El concepto matemático en el que se basa el modelo de regresión es una relación de dependencia entre una variable denominada dependiente denotada generalment por<strong>Y</strong>, y un conjunto de variables explicativas o independientes, denotadas por el vector X que incluye las variabkes explicativas individuales <span class="math inline">\(x_i\)</span>.</p>
<p>En términos matemáticos la siguiente ecuación expresa la forma básica del modelo <span class="math display">\[Y=a+bX+e\]</span></p>
<p>Note que esta ecuación representa una recta.</p>
<p><strong>Recomendación</strong> <a href="https://drive.google.com/file/d/1peEQsZpHYhGscI6PGUQJ5gCFrM59sT35/view?usp=sharing">Linear regression Analysis.Xin Yan, 2009</a></p>
<p>El proceso de modelar la relación entre la variable dependiente y las explicativas incluye la determinación de la <strong>significancia estadística</strong> esto es, el grado de confianza sobre que tanto la forma funcional utilizada reproduce la verdadera forma funcional que subyace a los datos.</p>
<p>El análisis de regresión es un proceso que permite <strong>predecir</strong> los valores de la variable de respuesta con base en los valores que toman las variables explicativas o indepedientes.</p>
<p>Este proceso se realiza mediante la estimación de los <strong>parámetros</strong> de la función lineal seleccionada y a partir del criterio estadística de referencia. En el particular, OLE.</p>
<p>Es una <strong>técnica de estimación paramétrica</strong>. En contraste regresiones de tipo no parametrico donde el objetivo no es estimar los parámetros de la función que describe la relación entre las variables del modelo.</p>
</div>
</div>
</div>
<div id="característcas-y-utilidad-de-los-modelos-de-regresión." class="section level2">
<h2>Característcas y utilidad de los modelos de regresión.</h2>
<p>Uno de los rasgos de interés de la técnica de <strong>regresión</strong> desde la perspectiva de su aplicación es su capacidad para hacer <strong>predicción</strong> y su uso para hacer inferencia, por ejemplo cuando el objetivo es determinar qué variables se relacionan con la variable de respuesta de interés o qué relación existe entre la variable dependiente y cada una de las variables explicativas.</p>
<p>Desde la determinación de indicadores en el contexto del análisis de política públíca en áreas como sociología, salud, economía o bien cuestiones como mortalidad por diabetes, migración y mercados laborales, el efecto de los gastos de camapaña sobre los resultados de votaciones, los efectos del presupuesto en educación sobre el desempeño educativo,etc,.</p>
<p>Para responder las preguntas de investigación de interés la formulación de un modelo (expresión matématica de las relaciones entre las variables que nos interesan) es útil.</p>
<p>El énfasis en los modelos presentados en nuestro curso es en <strong>datos no experimentales</strong>,(Un tipo de datos común en ciencias sociales). Datos que no se generan mediante experimentos contralados sobre individuos, este ultimo tipo común en otras disciplinas como Biología, ciencias de la salud con los ensayos clínicos por ejemplo.</p>
</div>
<div id="modelos-y-relación-funcional-entre-variables." class="section level2">
<h2>Modelos y relación funcional entre variables.</h2>
<p>Una forma de representar la relación entre dos variables es a través de un diagrama de dispersión. La siguiente gráfica presenta la realación entre dos variables años de educación y salario para una muestra de 526 individuos.</p>
<pre><code>## &#39;data.frame&#39;:    526 obs. of  24 variables:
##  $ wage    : num  3.1 3.24 3 6 5.3 ...
##  $ educ    : int  11 12 11 8 12 16 18 12 12 17 ...
##  $ exper   : int  2 22 2 44 7 9 15 5 26 22 ...
##  $ tenure  : int  0 2 0 28 2 8 7 3 4 21 ...
##  $ nonwhite: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ female  : int  1 1 0 0 0 0 0 1 1 0 ...
##  $ married : int  0 1 0 1 1 1 0 0 0 1 ...
##  $ numdep  : int  2 3 2 0 1 0 0 0 2 0 ...
##  $ smsa    : int  1 1 0 1 0 1 1 1 1 1 ...
##  $ northcen: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ south   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ west    : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ construc: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ ndurman : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ trcommpu: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ trade   : int  0 0 1 0 0 0 1 0 1 0 ...
##  $ services: int  0 1 0 0 0 0 0 0 0 0 ...
##  $ profserv: int  0 0 0 0 0 1 0 0 0 0 ...
##  $ profocc : int  0 0 0 0 0 1 1 1 1 1 ...
##  $ clerocc : int  0 0 0 1 0 0 0 0 0 0 ...
##  $ servocc : int  0 1 0 0 0 0 0 0 0 0 ...
##  $ lwage   : num  1.13 1.18 1.1 1.79 1.67 ...
##  $ expersq : int  4 484 4 1936 49 81 225 25 676 484 ...
##  $ tenursq : int  0 4 0 784 4 64 49 9 16 441 ...
##  - attr(*, &quot;time.stamp&quot;)= chr &quot;25 Jun 2011 23:03&quot;</code></pre>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-1-1.png" width="768" /></p>
<p>A continuación se muestran posibles variables explicativas respecto a la variable dependiente salario, utilizando un scatter plot y una linea de tendencia generada mediante un modelo de regresión lineal.</p>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-2-1.png" width="960" /></p>
<p>Note que para cada variable se observa una de posible relación positiva de distinta magnitud.</p>
<p>El siguiente ejemplo presenta la relación de tres posibles variables explicativas para una variable dependiente denominada ventas.</p>
<p>La gráfica muestra la relación entre la variable dependiente ventas (expresada en miles) y los gastos en publicidad en tres diferentes medios (variables independientes, expresada en miles de USD). n=200.</p>
<p><img src="/img/r1.jpg" /></p>
<p>Cada linea azul representa un modelo que puede usarse para predecir la variable <strong>dependiente</strong> <strong>ventas</strong> usando el procedimiento de mínimos cuadrados (ordinary least squres <strong>OLS</strong>).</p>
<p>En términos de notación la relación previa puede generalizarse como:</p>
<p><span class="math display">\[Y=f(X)+\epsilon\]</span>
Aquí <span class="math inline">\(\epsilon\)</span> representa un error aleatorio y <span class="math inline">\(f()\)</span> representa la forma funcional que caracteriza la relación entre la variable dependiente y las variables explicativas. Esta forma funcional se <strong>asume lineal</strong> en el modelo de regresión que estudiamos en esta sesión.</p>
<p>Consideremos el siguiente ejemplo sobre el mercado laboral en el que se trata de analizar el efecto que determinantes como la educación, la experiencia o el nivel de entrenamiento tienen sobre la productividad.</p>
<p>En términos teóricos se espera que estos determinantes tengan un impacto sobre la productividad y considerado la relación positiva entre la productividad y el salario, se prevé un impacto en este último.</p>
<p><span class="math display">\[y=f(x_1, x_2, x_3)\]</span>
<span class="math display">\[ salario=f(educ, exper, training)\]</span></p>
<p>Donde:</p>
<p>Salario: remuneración por hora.</p>
<p>educ: Años de educación formal.</p>
<p>exper: Años de experiencia en el trabajo.</p>
<p>training: semanas dedicadas a entrenamiento.</p>
<p>Note el tipo de variables utilizadas, en este caso son numéricas (<strong>cuantitativas</strong> continuas), si bien en este curso el énfasis está en el uso de variables cuantitativas, es posible tambien incluir variables categóricas (<em>factors</em> que registran información <strong>cualitativa</strong> como sexo, la pertenencia a una región, etc.)</p>
<p><span class="math display">\[salario=\beta_0+\beta_1 educ +\beta_2 exper +\beta_3 training +\epsilon \]</span></p>
<p><img src="/img/lm1.jpg" /></p>
<p><strong>Note</strong> en este caso que la relación verdadera entre las variables aunque se asume lineal puede tener otra forma funcional. Este aspecto es importante por que la precisión de las estimaciones realizadas depende de la selección de la forma funcional adecuada.</p>
<p><strong>Note</strong> La presencia de observaciones por arriba y algunas por abajo de la linea que expresa la relación funcional. Esto implica la presencia de errores respecto de medición entre la forma funcional y el valor real de los datos (estos errores son expresión de la naturaleza estocástica de las observaciones en la muestra).</p>
<p>En términos formales la variación que observamos en la gráfica se puede expresar como:</p>
<p><img src="/img/lm2.jpg" /></p>
<p>La <strong>precisión</strong> en las predicciones obtenidas mediante un modelo, es decir la variación entre los valores estimados <span class="math inline">\(\hat{Y} =f(X)\)</span> y el valor real de la variable <span class="math inline">\(Y\)</span> se integra fundamentalmente por dos partes:</p>
<p>La variación <strong>reductible</strong>, que se genera por la diferencia entre la forma funcional real y la forma funcional estimada por el modelo.</p>
<p>Y la variación <strong>irreductible</strong> atribuida a la naturaleza estocástica de las variables estudiadas que se ven influenciadas por un número infinito de factores mismos que no se capturan por el modelo.</p>
<p>Note la importancia de la herramienta gráfica como un paso preliminar a la construcción del modelo y en ayuda para conocer la relación posible entre las variables explicativas usadas como regresores y la variable dependiente que deseamos predecir o hacer inferencia.</p>
<p>Como primer paso una herramienta gráfica fundamental del análisis de regresión es la representación de los datos mediante un <strong>scatter plot</strong> bidimensional.</p>
<p>En algunos casos la relación entre la variable dependiente y las variables explicativas se basa en una teoría establecida con un grado elevado de concenso y precisión, no obstante en otros casos la relación entre variables se puede inferir a partir de su comportamiento, en cualquier caso las gráficas de dispersión son de suma importancia para explorar la relación entre las variables del modelo propuesto.</p>
<p>Lectura <strong>Recomendada</strong> <a href="https://drive.google.com/file/d/1pFd7mqYVSZ2qgNNIdZGUw51O1Cp5Y81P/view?usp=sharing">Sanford Weisberg ,2010</a></p>
</div>
<div id="sobre-el-proceso-de-modelado" class="section level2">
<h2>Sobre el proceso de modelado</h2>
<p>El primer paso es seleccionar al forma funcional que vincula a la variable dependiente con las explicativas. En esta caso el modelo que estudiaremos primero es cuando <span class="math inline">\(f\)</span> es lineal y bajo la estimación paramétrica.</p>
<p><span class="math display">\[f(X)=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ...+\beta_p X_p \]</span></p>
<p>Este es un modelo lineal con <span class="math inline">\(p+1\)</span> parámetros a estimar, (uno por cada variable explicativa y el intercepto.)</p>
<p>El objetivo es estimar los parámetros con base en la forma funcional tal que se aproxime lo mas posible a la forma funcional real. Para lo cual un <strong>método paramétrico clásico</strong> es el de <strong>minimos cuadrados</strong> (OLS).</p>
<p><span class="math display">\[Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ...+\beta_p X_p \]</span>
Una desventaja potencial del este <strong>método paramétrico</strong> es que la pre-definición de la forma funcional limita la precisión en comparación a estimaciones con métodos no paramétricos.</p>
<p>La siguiente representación gráfica corresponde al caso de la forma funcional lineal estimada con métodos paramétricos mediante MCO a), b) la forma funcional real y la estimación de un modelo no-parámetrico en c).</p>
<p><img src="/img/lm3.jpg" /></p>
<p>La desventaja en métodos no-parámetricos es que su misma precisión implica cierta rigidez para su replica en muestras distintas o con observaciones nuevas.</p>
<p>El punto en la selección del modelo empleado para representar la forma funcional es el <strong>balance entre interpretabilidad y precisión</strong> (entendiendo la precisión respecto a la flexibilidad para reproducir diversas formas funcionales no solo formas lineales como es el caso del método de RL por MCO.).</p>
<p>De hecho el modelo de regresión lineal es conveniente cuando buscamos <strong>hacer inferencias</strong> dada su relativa facilidad de interpretación.</p>
<p>En síntesis es importante tener en mente que hay un <strong>trade off</strong> entre el nivel de precisión en los estimados que pueden generarse por el modelo (el grado de sesgo <em>Bias</em> ej. Qué tanto nuestra forma funcional se aproxima a la forma funcional real que subyace a los datos) y la varianza, entendida esta como la magnitud en la que <span class="math inline">\(\hat{Y}\)</span> cambia con la estimación de otras muestras o bases de datos.</p>
<div id="preguntas-relevantes-en-las-que-el-análisis-de-regresión-lineal-puede-ser-útil." class="section level3">
<h3>Preguntas relevantes en las que el análisis de regresión lineal puede ser útil.</h3>
<p>Sea <span class="math display">\[y = \beta_0 + \beta_1 x +\epsilon\]</span> un modelo lineal.</p>
<p>1.-Exsiste una relación entre las variables propuestas. Ej. salario y años de educación, peso del recien nacido y fumar durante el embarazo, ventas y gastos en publicidad para diferentes medios, flujos migratorios y características de los estados (USA), etc,.?</p>
<p>En este sentido nuestra meta es <strong>proporcionar evidencia</strong> de la existencia de una relación entre variable dependiente y explicativas.</p>
<p>2.- ¿Qué tan fuerte es la relación, entre las variables estudiadas. ¿Cuál es la magnitud de este grado de asociación en caso de existir?</p>
<p>Nos permite esta asociación hacer predicciones con un alto grado de exactitud? O la predicción es solo ligeramente mejor que una hecha de manera aleatoria.</p>
<p>3.- ¿De entre un conjunto de posibles variables explicativas ¿cuál variable contribuye con mayor peso para el comportamiento de la variable dependiente.</p>
<p>Para responder esta pregunta necesitamos determinar de los efectos individuales para cada variable.</p>
<p>4.¿Qué tan preciso es nuestro estimado del efecto individual que las variables explicativas tienen sobre la variable dependiente? Para un incremento de una unidad en las variables independientes ¿cuál es el efecto en al variale dependiente.</p>
<p>Suponiendo.</p>
<p><span class="math display">\[\Delta Y=\beta_1 \Delta x,  si \Delta \epsilon= 0\]</span>
Donde <span class="math inline">\(\beta_1\)</span> es el parámetro que indica la <strong>pendiente</strong> en la relación entre y, x.</p>
<p>Note que <span class="math inline">\(\Delta \epsilon=0\)</span> es un supuesto que refleja el hecho de que los otros factores, que de hecho no se miden, son constantes.</p>
<p>5.- ¿Cuál es el nivel de precisión en la predicción de la variable dependiente?</p>
<p>Para un nivel dado de las variables independientes ¿cuál es nuestra predicción de la variable dependiente?</p>
<p>6.- ¿Es la relación entre las variables lineal? Tal vez una combinación en las variables independientes tiene un mayor efecto en la variable dependiente que el incremento por separado en cada una de las variables.</p>
<p>Ej. La experiencia puede impactar las habilidades de un año a otro en forma creciente o tener un mayor imapcto considerando una combinación de educación y experiencia. Este aspecto implica la estimación de <em>interacciones entre las variables</em></p>
<p>Leer Capítulo 3. <a href="https://drive.google.com/file/d/1zaKc1XWvELsUgZrS32QdpHoM4-w0y7Qx/view?usp=sharing">An Introduction to Statistical Learning</a>.</p>
<div id="ejemplo" class="section level4">
<h4>Ejemplo</h4>
<p>Buscamos predecir el valor medio de las viviendas (Y) con base en 12 variables independientes o regresores (x). La base de datos cuenta con 560 observaciones correspondientes a información en una ciudad.</p>
<pre class="r"><code>library(ISLR2)</code></pre>
<pre><code>## Warning: package &#39;ISLR2&#39; was built under R version 4.1.3</code></pre>
<pre class="r"><code>data(Boston, package=&quot;MASS&quot;)

Boston %&gt;% 
  select(crim, lstat, indus, nox, ptratio, rm, tax, dis, age)</code></pre>
<pre><code>##         crim lstat indus    nox ptratio    rm tax     dis   age
## 1    0.00632  4.98  2.31 0.5380    15.3 6.575 296  4.0900  65.2
## 2    0.02731  9.14  7.07 0.4690    17.8 6.421 242  4.9671  78.9
## 3    0.02729  4.03  7.07 0.4690    17.8 7.185 242  4.9671  61.1
## 4    0.03237  2.94  2.18 0.4580    18.7 6.998 222  6.0622  45.8
## 5    0.06905  5.33  2.18 0.4580    18.7 7.147 222  6.0622  54.2
## 6    0.02985  5.21  2.18 0.4580    18.7 6.430 222  6.0622  58.7
## 7    0.08829 12.43  7.87 0.5240    15.2 6.012 311  5.5605  66.6
## 8    0.14455 19.15  7.87 0.5240    15.2 6.172 311  5.9505  96.1
## 9    0.21124 29.93  7.87 0.5240    15.2 5.631 311  6.0821 100.0
## 10   0.17004 17.10  7.87 0.5240    15.2 6.004 311  6.5921  85.9
## 11   0.22489 20.45  7.87 0.5240    15.2 6.377 311  6.3467  94.3
## 12   0.11747 13.27  7.87 0.5240    15.2 6.009 311  6.2267  82.9
## 13   0.09378 15.71  7.87 0.5240    15.2 5.889 311  5.4509  39.0
## 14   0.62976  8.26  8.14 0.5380    21.0 5.949 307  4.7075  61.8
## 15   0.63796 10.26  8.14 0.5380    21.0 6.096 307  4.4619  84.5
## 16   0.62739  8.47  8.14 0.5380    21.0 5.834 307  4.4986  56.5
## 17   1.05393  6.58  8.14 0.5380    21.0 5.935 307  4.4986  29.3
## 18   0.78420 14.67  8.14 0.5380    21.0 5.990 307  4.2579  81.7
## 19   0.80271 11.69  8.14 0.5380    21.0 5.456 307  3.7965  36.6
## 20   0.72580 11.28  8.14 0.5380    21.0 5.727 307  3.7965  69.5
## 21   1.25179 21.02  8.14 0.5380    21.0 5.570 307  3.7979  98.1
## 22   0.85204 13.83  8.14 0.5380    21.0 5.965 307  4.0123  89.2
## 23   1.23247 18.72  8.14 0.5380    21.0 6.142 307  3.9769  91.7
## 24   0.98843 19.88  8.14 0.5380    21.0 5.813 307  4.0952 100.0
## 25   0.75026 16.30  8.14 0.5380    21.0 5.924 307  4.3996  94.1
## 26   0.84054 16.51  8.14 0.5380    21.0 5.599 307  4.4546  85.7
## 27   0.67191 14.81  8.14 0.5380    21.0 5.813 307  4.6820  90.3
## 28   0.95577 17.28  8.14 0.5380    21.0 6.047 307  4.4534  88.8
## 29   0.77299 12.80  8.14 0.5380    21.0 6.495 307  4.4547  94.4
## 30   1.00245 11.98  8.14 0.5380    21.0 6.674 307  4.2390  87.3
## 31   1.13081 22.60  8.14 0.5380    21.0 5.713 307  4.2330  94.1
## 32   1.35472 13.04  8.14 0.5380    21.0 6.072 307  4.1750 100.0
## 33   1.38799 27.71  8.14 0.5380    21.0 5.950 307  3.9900  82.0
## 34   1.15172 18.35  8.14 0.5380    21.0 5.701 307  3.7872  95.0
## 35   1.61282 20.34  8.14 0.5380    21.0 6.096 307  3.7598  96.9
## 36   0.06417  9.68  5.96 0.4990    19.2 5.933 279  3.3603  68.2
## 37   0.09744 11.41  5.96 0.4990    19.2 5.841 279  3.3779  61.4
## 38   0.08014  8.77  5.96 0.4990    19.2 5.850 279  3.9342  41.5
## 39   0.17505 10.13  5.96 0.4990    19.2 5.966 279  3.8473  30.2
## 40   0.02763  4.32  2.95 0.4280    18.3 6.595 252  5.4011  21.8
## 41   0.03359  1.98  2.95 0.4280    18.3 7.024 252  5.4011  15.8
## 42   0.12744  4.84  6.91 0.4480    17.9 6.770 233  5.7209   2.9
## 43   0.14150  5.81  6.91 0.4480    17.9 6.169 233  5.7209   6.6
## 44   0.15936  7.44  6.91 0.4480    17.9 6.211 233  5.7209   6.5
## 45   0.12269  9.55  6.91 0.4480    17.9 6.069 233  5.7209  40.0
## 46   0.17142 10.21  6.91 0.4480    17.9 5.682 233  5.1004  33.8
## 47   0.18836 14.15  6.91 0.4480    17.9 5.786 233  5.1004  33.3
## 48   0.22927 18.80  6.91 0.4480    17.9 6.030 233  5.6894  85.5
## 49   0.25387 30.81  6.91 0.4480    17.9 5.399 233  5.8700  95.3
## 50   0.21977 16.20  6.91 0.4480    17.9 5.602 233  6.0877  62.0
## 51   0.08873 13.45  5.64 0.4390    16.8 5.963 243  6.8147  45.7
## 52   0.04337  9.43  5.64 0.4390    16.8 6.115 243  6.8147  63.0
## 53   0.05360  5.28  5.64 0.4390    16.8 6.511 243  6.8147  21.1
## 54   0.04981  8.43  5.64 0.4390    16.8 5.998 243  6.8147  21.4
## 55   0.01360 14.80  4.00 0.4100    21.1 5.888 469  7.3197  47.6
## 56   0.01311  4.81  1.22 0.4030    17.9 7.249 226  8.6966  21.9
## 57   0.02055  5.77  0.74 0.4100    17.3 6.383 313  9.1876  35.7
## 58   0.01432  3.95  1.32 0.4110    15.1 6.816 256  8.3248  40.5
## 59   0.15445  6.86  5.13 0.4530    19.7 6.145 284  7.8148  29.2
## 60   0.10328  9.22  5.13 0.4530    19.7 5.927 284  6.9320  47.2
## 61   0.14932 13.15  5.13 0.4530    19.7 5.741 284  7.2254  66.2
## 62   0.17171 14.44  5.13 0.4530    19.7 5.966 284  6.8185  93.4
## 63   0.11027  6.73  5.13 0.4530    19.7 6.456 284  7.2255  67.8
## 64   0.12650  9.50  5.13 0.4530    19.7 6.762 284  7.9809  43.4
## 65   0.01951  8.05  1.38 0.4161    18.6 7.104 216  9.2229  59.5
## 66   0.03584  4.67  3.37 0.3980    16.1 6.290 337  6.6115  17.8
## 67   0.04379 10.24  3.37 0.3980    16.1 5.787 337  6.6115  31.1
## 68   0.05789  8.10  6.07 0.4090    18.9 5.878 345  6.4980  21.4
## 69   0.13554 13.09  6.07 0.4090    18.9 5.594 345  6.4980  36.8
## 70   0.12816  8.79  6.07 0.4090    18.9 5.885 345  6.4980  33.0
## 71   0.08826  6.72 10.81 0.4130    19.2 6.417 305  5.2873   6.6
## 72   0.15876  9.88 10.81 0.4130    19.2 5.961 305  5.2873  17.5
## 73   0.09164  5.52 10.81 0.4130    19.2 6.065 305  5.2873   7.8
## 74   0.19539  7.54 10.81 0.4130    19.2 6.245 305  5.2873   6.2
## 75   0.07896  6.78 12.83 0.4370    18.7 6.273 398  4.2515   6.0
## 76   0.09512  8.94 12.83 0.4370    18.7 6.286 398  4.5026  45.0
## 77   0.10153 11.97 12.83 0.4370    18.7 6.279 398  4.0522  74.5
## 78   0.08707 10.27 12.83 0.4370    18.7 6.140 398  4.0905  45.8
## 79   0.05646 12.34 12.83 0.4370    18.7 6.232 398  5.0141  53.7
## 80   0.08387  9.10 12.83 0.4370    18.7 5.874 398  4.5026  36.6
## 81   0.04113  5.29  4.86 0.4260    19.0 6.727 281  5.4007  33.5
## 82   0.04462  7.22  4.86 0.4260    19.0 6.619 281  5.4007  70.4
## 83   0.03659  6.72  4.86 0.4260    19.0 6.302 281  5.4007  32.2
## 84   0.03551  7.51  4.86 0.4260    19.0 6.167 281  5.4007  46.7
## 85   0.05059  9.62  4.49 0.4490    18.5 6.389 247  4.7794  48.0
## 86   0.05735  6.53  4.49 0.4490    18.5 6.630 247  4.4377  56.1
## 87   0.05188 12.86  4.49 0.4490    18.5 6.015 247  4.4272  45.1
## 88   0.07151  8.44  4.49 0.4490    18.5 6.121 247  3.7476  56.8
## 89   0.05660  5.50  3.41 0.4890    17.8 7.007 270  3.4217  86.3
## 90   0.05302  5.70  3.41 0.4890    17.8 7.079 270  3.4145  63.1
## 91   0.04684  8.81  3.41 0.4890    17.8 6.417 270  3.0923  66.1
## 92   0.03932  8.20  3.41 0.4890    17.8 6.405 270  3.0921  73.9
## 93   0.04203  8.16 15.04 0.4640    18.2 6.442 270  3.6659  53.6
## 94   0.02875  6.21 15.04 0.4640    18.2 6.211 270  3.6659  28.9
## 95   0.04294 10.59 15.04 0.4640    18.2 6.249 270  3.6150  77.3
## 96   0.12204  6.65  2.89 0.4450    18.0 6.625 276  3.4952  57.8
## 97   0.11504 11.34  2.89 0.4450    18.0 6.163 276  3.4952  69.6
## 98   0.12083  4.21  2.89 0.4450    18.0 8.069 276  3.4952  76.0
## 99   0.08187  3.57  2.89 0.4450    18.0 7.820 276  3.4952  36.9
## 100  0.06860  6.19  2.89 0.4450    18.0 7.416 276  3.4952  62.5
## 101  0.14866  9.42  8.56 0.5200    20.9 6.727 384  2.7778  79.9
## 102  0.11432  7.67  8.56 0.5200    20.9 6.781 384  2.8561  71.3
## 103  0.22876 10.63  8.56 0.5200    20.9 6.405 384  2.7147  85.4
## 104  0.21161 13.44  8.56 0.5200    20.9 6.137 384  2.7147  87.4
## 105  0.13960 12.33  8.56 0.5200    20.9 6.167 384  2.4210  90.0
## 106  0.13262 16.47  8.56 0.5200    20.9 5.851 384  2.1069  96.7
## 107  0.17120 18.66  8.56 0.5200    20.9 5.836 384  2.2110  91.9
## 108  0.13117 14.09  8.56 0.5200    20.9 6.127 384  2.1224  85.2
## 109  0.12802 12.27  8.56 0.5200    20.9 6.474 384  2.4329  97.1
## 110  0.26363 15.55  8.56 0.5200    20.9 6.229 384  2.5451  91.2
## 111  0.10793 13.00  8.56 0.5200    20.9 6.195 384  2.7778  54.4
## 112  0.10084 10.16 10.01 0.5470    17.8 6.715 432  2.6775  81.6
## 113  0.12329 16.21 10.01 0.5470    17.8 5.913 432  2.3534  92.9
## 114  0.22212 17.09 10.01 0.5470    17.8 6.092 432  2.5480  95.4
## 115  0.14231 10.45 10.01 0.5470    17.8 6.254 432  2.2565  84.2
## 116  0.17134 15.76 10.01 0.5470    17.8 5.928 432  2.4631  88.2
## 117  0.13158 12.04 10.01 0.5470    17.8 6.176 432  2.7301  72.5
## 118  0.15098 10.30 10.01 0.5470    17.8 6.021 432  2.7474  82.6
## 119  0.13058 15.37 10.01 0.5470    17.8 5.872 432  2.4775  73.1
## 120  0.14476 13.61 10.01 0.5470    17.8 5.731 432  2.7592  65.2
## 121  0.06899 14.37 25.65 0.5810    19.1 5.870 188  2.2577  69.7
## 122  0.07165 14.27 25.65 0.5810    19.1 6.004 188  2.1974  84.1
## 123  0.09299 17.93 25.65 0.5810    19.1 5.961 188  2.0869  92.9
## 124  0.15038 25.41 25.65 0.5810    19.1 5.856 188  1.9444  97.0
## 125  0.09849 17.58 25.65 0.5810    19.1 5.879 188  2.0063  95.8
## 126  0.16902 14.81 25.65 0.5810    19.1 5.986 188  1.9929  88.4
## 127  0.38735 27.26 25.65 0.5810    19.1 5.613 188  1.7572  95.6
## 128  0.25915 17.19 21.89 0.6240    21.2 5.693 437  1.7883  96.0
## 129  0.32543 15.39 21.89 0.6240    21.2 6.431 437  1.8125  98.8
## 130  0.88125 18.34 21.89 0.6240    21.2 5.637 437  1.9799  94.7
## 131  0.34006 12.60 21.89 0.6240    21.2 6.458 437  2.1185  98.9
## 132  1.19294 12.26 21.89 0.6240    21.2 6.326 437  2.2710  97.7
## 133  0.59005 11.12 21.89 0.6240    21.2 6.372 437  2.3274  97.9
## 134  0.32982 15.03 21.89 0.6240    21.2 5.822 437  2.4699  95.4
## 135  0.97617 17.31 21.89 0.6240    21.2 5.757 437  2.3460  98.4
## 136  0.55778 16.96 21.89 0.6240    21.2 6.335 437  2.1107  98.2
## 137  0.32264 16.90 21.89 0.6240    21.2 5.942 437  1.9669  93.5
## 138  0.35233 14.59 21.89 0.6240    21.2 6.454 437  1.8498  98.4
## 139  0.24980 21.32 21.89 0.6240    21.2 5.857 437  1.6686  98.2
## 140  0.54452 18.46 21.89 0.6240    21.2 6.151 437  1.6687  97.9
## 141  0.29090 24.16 21.89 0.6240    21.2 6.174 437  1.6119  93.6
## 142  1.62864 34.41 21.89 0.6240    21.2 5.019 437  1.4394 100.0
## 143  3.32105 26.82 19.58 0.8710    14.7 5.403 403  1.3216 100.0
## 144  4.09740 26.42 19.58 0.8710    14.7 5.468 403  1.4118 100.0
## 145  2.77974 29.29 19.58 0.8710    14.7 4.903 403  1.3459  97.8
## 146  2.37934 27.80 19.58 0.8710    14.7 6.130 403  1.4191 100.0
## 147  2.15505 16.65 19.58 0.8710    14.7 5.628 403  1.5166 100.0
## 148  2.36862 29.53 19.58 0.8710    14.7 4.926 403  1.4608  95.7
## 149  2.33099 28.32 19.58 0.8710    14.7 5.186 403  1.5296  93.8
## 150  2.73397 21.45 19.58 0.8710    14.7 5.597 403  1.5257  94.9
## 151  1.65660 14.10 19.58 0.8710    14.7 6.122 403  1.6180  97.3
## 152  1.49632 13.28 19.58 0.8710    14.7 5.404 403  1.5916 100.0
## 153  1.12658 12.12 19.58 0.8710    14.7 5.012 403  1.6102  88.0
## 154  2.14918 15.79 19.58 0.8710    14.7 5.709 403  1.6232  98.5
## 155  1.41385 15.12 19.58 0.8710    14.7 6.129 403  1.7494  96.0
## 156  3.53501 15.02 19.58 0.8710    14.7 6.152 403  1.7455  82.6
## 157  2.44668 16.14 19.58 0.8710    14.7 5.272 403  1.7364  94.0
## 158  1.22358  4.59 19.58 0.6050    14.7 6.943 403  1.8773  97.4
## 159  1.34284  6.43 19.58 0.6050    14.7 6.066 403  1.7573 100.0
## 160  1.42502  7.39 19.58 0.8710    14.7 6.510 403  1.7659 100.0
## 161  1.27346  5.50 19.58 0.6050    14.7 6.250 403  1.7984  92.6
## 162  1.46336  1.73 19.58 0.6050    14.7 7.489 403  1.9709  90.8
## 163  1.83377  1.92 19.58 0.6050    14.7 7.802 403  2.0407  98.2
## 164  1.51902  3.32 19.58 0.6050    14.7 8.375 403  2.1620  93.9
## 165  2.24236 11.64 19.58 0.6050    14.7 5.854 403  2.4220  91.8
## 166  2.92400  9.81 19.58 0.6050    14.7 6.101 403  2.2834  93.0
## 167  2.01019  3.70 19.58 0.6050    14.7 7.929 403  2.0459  96.2
## 168  1.80028 12.14 19.58 0.6050    14.7 5.877 403  2.4259  79.2
## 169  2.30040 11.10 19.58 0.6050    14.7 6.319 403  2.1000  96.1
## 170  2.44953 11.32 19.58 0.6050    14.7 6.402 403  2.2625  95.2
## 171  1.20742 14.43 19.58 0.6050    14.7 5.875 403  2.4259  94.6
## 172  2.31390 12.03 19.58 0.6050    14.7 5.880 403  2.3887  97.3
## 173  0.13914 14.69  4.05 0.5100    16.6 5.572 296  2.5961  88.5
## 174  0.09178  9.04  4.05 0.5100    16.6 6.416 296  2.6463  84.1
## 175  0.08447  9.64  4.05 0.5100    16.6 5.859 296  2.7019  68.7
## 176  0.06664  5.33  4.05 0.5100    16.6 6.546 296  3.1323  33.1
## 177  0.07022 10.11  4.05 0.5100    16.6 6.020 296  3.5549  47.2
## 178  0.05425  6.29  4.05 0.5100    16.6 6.315 296  3.3175  73.4
## 179  0.06642  6.92  4.05 0.5100    16.6 6.860 296  2.9153  74.4
## 180  0.05780  5.04  2.46 0.4880    17.8 6.980 193  2.8290  58.4
## 181  0.06588  7.56  2.46 0.4880    17.8 7.765 193  2.7410  83.3
## 182  0.06888  9.45  2.46 0.4880    17.8 6.144 193  2.5979  62.2
## 183  0.09103  4.82  2.46 0.4880    17.8 7.155 193  2.7006  92.2
## 184  0.10008  5.68  2.46 0.4880    17.8 6.563 193  2.8470  95.6
## 185  0.08308 13.98  2.46 0.4880    17.8 5.604 193  2.9879  89.8
## 186  0.06047 13.15  2.46 0.4880    17.8 6.153 193  3.2797  68.8
## 187  0.05602  4.45  2.46 0.4880    17.8 7.831 193  3.1992  53.6
## 188  0.07875  6.68  3.44 0.4370    15.2 6.782 398  3.7886  41.1
## 189  0.12579  4.56  3.44 0.4370    15.2 6.556 398  4.5667  29.1
## 190  0.08370  5.39  3.44 0.4370    15.2 7.185 398  4.5667  38.9
## 191  0.09068  5.10  3.44 0.4370    15.2 6.951 398  6.4798  21.5
## 192  0.06911  4.69  3.44 0.4370    15.2 6.739 398  6.4798  30.8
## 193  0.08664  2.87  3.44 0.4370    15.2 7.178 398  6.4798  26.3
## 194  0.02187  5.03  2.93 0.4010    15.6 6.800 265  6.2196   9.9
## 195  0.01439  4.38  2.93 0.4010    15.6 6.604 265  6.2196  18.8
## 196  0.01381  2.97  0.46 0.4220    14.4 7.875 255  5.6484  32.0
## 197  0.04011  4.08  1.52 0.4040    12.6 7.287 329  7.3090  34.1
## 198  0.04666  8.61  1.52 0.4040    12.6 7.107 329  7.3090  36.6
## 199  0.03768  6.62  1.52 0.4040    12.6 7.274 329  7.3090  38.3
## 200  0.03150  4.56  1.47 0.4030    17.0 6.975 402  7.6534  15.3
## 201  0.01778  4.45  1.47 0.4030    17.0 7.135 402  7.6534  13.9
## 202  0.03445  7.43  2.03 0.4150    14.7 6.162 348  6.2700  38.4
## 203  0.02177  3.11  2.03 0.4150    14.7 7.610 348  6.2700  15.7
## 204  0.03510  3.81  2.68 0.4161    14.7 7.853 224  5.1180  33.2
## 205  0.02009  2.88  2.68 0.4161    14.7 8.034 224  5.1180  31.9
## 206  0.13642 10.87 10.59 0.4890    18.6 5.891 277  3.9454  22.3
## 207  0.22969 10.97 10.59 0.4890    18.6 6.326 277  4.3549  52.5
## 208  0.25199 18.06 10.59 0.4890    18.6 5.783 277  4.3549  72.7
## 209  0.13587 14.66 10.59 0.4890    18.6 6.064 277  4.2392  59.1
## 210  0.43571 23.09 10.59 0.4890    18.6 5.344 277  3.8750 100.0
## 211  0.17446 17.27 10.59 0.4890    18.6 5.960 277  3.8771  92.1
## 212  0.37578 23.98 10.59 0.4890    18.6 5.404 277  3.6650  88.6
## 213  0.21719 16.03 10.59 0.4890    18.6 5.807 277  3.6526  53.8
## 214  0.14052  9.38 10.59 0.4890    18.6 6.375 277  3.9454  32.3
## 215  0.28955 29.55 10.59 0.4890    18.6 5.412 277  3.5875   9.8
## 216  0.19802  9.47 10.59 0.4890    18.6 6.182 277  3.9454  42.4
## 217  0.04560 13.51 13.89 0.5500    16.4 5.888 276  3.1121  56.0
## 218  0.07013  9.69 13.89 0.5500    16.4 6.642 276  3.4211  85.1
## 219  0.11069 17.92 13.89 0.5500    16.4 5.951 276  2.8893  93.8
## 220  0.11425 10.50 13.89 0.5500    16.4 6.373 276  3.3633  92.4
## 221  0.35809  9.71  6.20 0.5070    17.4 6.951 307  2.8617  88.5
## 222  0.40771 21.46  6.20 0.5070    17.4 6.164 307  3.0480  91.3
## 223  0.62356  9.93  6.20 0.5070    17.4 6.879 307  3.2721  77.7
## 224  0.61470  7.60  6.20 0.5070    17.4 6.618 307  3.2721  80.8
## 225  0.31533  4.14  6.20 0.5040    17.4 8.266 307  2.8944  78.3
## 226  0.52693  4.63  6.20 0.5040    17.4 8.725 307  2.8944  83.0
## 227  0.38214  3.13  6.20 0.5040    17.4 8.040 307  3.2157  86.5
## 228  0.41238  6.36  6.20 0.5040    17.4 7.163 307  3.2157  79.9
## 229  0.29819  3.92  6.20 0.5040    17.4 7.686 307  3.3751  17.0
## 230  0.44178  3.76  6.20 0.5040    17.4 6.552 307  3.3751  21.4
## 231  0.53700 11.65  6.20 0.5040    17.4 5.981 307  3.6715  68.1
## 232  0.46296  5.25  6.20 0.5040    17.4 7.412 307  3.6715  76.9
## 233  0.57529  2.47  6.20 0.5070    17.4 8.337 307  3.8384  73.3
## 234  0.33147  3.95  6.20 0.5070    17.4 8.247 307  3.6519  70.4
## 235  0.44791  8.05  6.20 0.5070    17.4 6.726 307  3.6519  66.5
## 236  0.33045 10.88  6.20 0.5070    17.4 6.086 307  3.6519  61.5
## 237  0.52058  9.54  6.20 0.5070    17.4 6.631 307  4.1480  76.5
## 238  0.51183  4.73  6.20 0.5070    17.4 7.358 307  4.1480  71.6
## 239  0.08244  6.36  4.93 0.4280    16.6 6.481 300  6.1899  18.5
## 240  0.09252  7.37  4.93 0.4280    16.6 6.606 300  6.1899  42.2
## 241  0.11329 11.38  4.93 0.4280    16.6 6.897 300  6.3361  54.3
## 242  0.10612 12.40  4.93 0.4280    16.6 6.095 300  6.3361  65.1
## 243  0.10290 11.22  4.93 0.4280    16.6 6.358 300  7.0355  52.9
## 244  0.12757  5.19  4.93 0.4280    16.6 6.393 300  7.0355   7.8
## 245  0.20608 12.50  5.86 0.4310    19.1 5.593 330  7.9549  76.5
## 246  0.19133 18.46  5.86 0.4310    19.1 5.605 330  7.9549  70.2
## 247  0.33983  9.16  5.86 0.4310    19.1 6.108 330  8.0555  34.9
## 248  0.19657 10.15  5.86 0.4310    19.1 6.226 330  8.0555  79.2
## 249  0.16439  9.52  5.86 0.4310    19.1 6.433 330  7.8265  49.1
## 250  0.19073  6.56  5.86 0.4310    19.1 6.718 330  7.8265  17.5
## 251  0.14030  5.90  5.86 0.4310    19.1 6.487 330  7.3967  13.0
## 252  0.21409  3.59  5.86 0.4310    19.1 6.438 330  7.3967   8.9
## 253  0.08221  3.53  5.86 0.4310    19.1 6.957 330  8.9067   6.8
## 254  0.36894  3.54  5.86 0.4310    19.1 8.259 330  8.9067   8.4
## 255  0.04819  6.57  3.64 0.3920    16.4 6.108 315  9.2203  32.0
## 256  0.03548  9.25  3.64 0.3920    16.4 5.876 315  9.2203  19.1
## 257  0.01538  3.11  3.75 0.3940    15.9 7.454 244  6.3361  34.2
## 258  0.61154  5.12  3.97 0.6470    13.0 8.704 264  1.8010  86.9
## 259  0.66351  7.79  3.97 0.6470    13.0 7.333 264  1.8946 100.0
## 260  0.65665  6.90  3.97 0.6470    13.0 6.842 264  2.0107 100.0
## 261  0.54011  9.59  3.97 0.6470    13.0 7.203 264  2.1121  81.8
## 262  0.53412  7.26  3.97 0.6470    13.0 7.520 264  2.1398  89.4
## 263  0.52014  5.91  3.97 0.6470    13.0 8.398 264  2.2885  91.5
## 264  0.82526 11.25  3.97 0.6470    13.0 7.327 264  2.0788  94.5
## 265  0.55007  8.10  3.97 0.6470    13.0 7.206 264  1.9301  91.6
## 266  0.76162 10.45  3.97 0.6470    13.0 5.560 264  1.9865  62.8
## 267  0.78570 14.79  3.97 0.6470    13.0 7.014 264  2.1329  84.6
## 268  0.57834  7.44  3.97 0.5750    13.0 8.297 264  2.4216  67.0
## 269  0.54050  3.16  3.97 0.5750    13.0 7.470 264  2.8720  52.6
## 270  0.09065 13.65  6.96 0.4640    18.6 5.920 223  3.9175  61.5
## 271  0.29916 13.00  6.96 0.4640    18.6 5.856 223  4.4290  42.1
## 272  0.16211  6.59  6.96 0.4640    18.6 6.240 223  4.4290  16.3
## 273  0.11460  7.73  6.96 0.4640    18.6 6.538 223  3.9175  58.7
## 274  0.22188  6.58  6.96 0.4640    18.6 7.691 223  4.3665  51.8
## 275  0.05644  3.53  6.41 0.4470    17.6 6.758 254  4.0776  32.9
## 276  0.09604  2.98  6.41 0.4470    17.6 6.854 254  4.2673  42.8
## 277  0.10469  6.05  6.41 0.4470    17.6 7.267 254  4.7872  49.0
## 278  0.06127  4.16  6.41 0.4470    17.6 6.826 254  4.8628  27.6
## 279  0.07978  7.19  6.41 0.4470    17.6 6.482 254  4.1403  32.1
## 280  0.21038  4.85  3.33 0.4429    14.9 6.812 216  4.1007  32.2
## 281  0.03578  3.76  3.33 0.4429    14.9 7.820 216  4.6947  64.5
## 282  0.03705  4.59  3.33 0.4429    14.9 6.968 216  5.2447  37.2
## 283  0.06129  3.01  3.33 0.4429    14.9 7.645 216  5.2119  49.7
## 284  0.01501  3.16  1.21 0.4010    13.6 7.923 198  5.8850  24.8
## 285  0.00906  7.85  2.97 0.4000    15.3 7.088 285  7.3073  20.8
## 286  0.01096  8.23  2.25 0.3890    15.3 6.453 300  7.3073  31.9
## 287  0.01965 12.93  1.76 0.3850    18.2 6.230 241  9.0892  31.5
## 288  0.03871  7.14  5.32 0.4050    16.6 6.209 293  7.3172  31.3
## 289  0.04590  7.60  5.32 0.4050    16.6 6.315 293  7.3172  45.6
## 290  0.04297  9.51  5.32 0.4050    16.6 6.565 293  7.3172  22.9
## 291  0.03502  3.33  4.95 0.4110    19.2 6.861 245  5.1167  27.9
## 292  0.07886  3.56  4.95 0.4110    19.2 7.148 245  5.1167  27.7
## 293  0.03615  4.70  4.95 0.4110    19.2 6.630 245  5.1167  23.4
## 294  0.08265  8.58 13.92 0.4370    16.0 6.127 289  5.5027  18.4
## 295  0.08199 10.40 13.92 0.4370    16.0 6.009 289  5.5027  42.3
## 296  0.12932  6.27 13.92 0.4370    16.0 6.678 289  5.9604  31.1
## 297  0.05372  7.39 13.92 0.4370    16.0 6.549 289  5.9604  51.0
## 298  0.14103 15.84 13.92 0.4370    16.0 5.790 289  6.3200  58.0
## 299  0.06466  4.97  2.24 0.4000    14.8 6.345 358  7.8278  20.1
## 300  0.05561  4.74  2.24 0.4000    14.8 7.041 358  7.8278  10.0
## 301  0.04417  6.07  2.24 0.4000    14.8 6.871 358  7.8278  47.4
## 302  0.03537  9.50  6.09 0.4330    16.1 6.590 329  5.4917  40.4
## 303  0.09266  8.67  6.09 0.4330    16.1 6.495 329  5.4917  18.4
## 304  0.10000  4.86  6.09 0.4330    16.1 6.982 329  5.4917  17.7
## 305  0.05515  6.93  2.18 0.4720    18.4 7.236 222  4.0220  41.1
## 306  0.05479  8.93  2.18 0.4720    18.4 6.616 222  3.3700  58.1
## 307  0.07503  6.47  2.18 0.4720    18.4 7.420 222  3.0992  71.9
## 308  0.04932  7.53  2.18 0.4720    18.4 6.849 222  3.1827  70.3
## 309  0.49298  4.54  9.90 0.5440    18.4 6.635 304  3.3175  82.5
## 310  0.34940  9.97  9.90 0.5440    18.4 5.972 304  3.1025  76.7
## 311  2.63548 12.64  9.90 0.5440    18.4 4.973 304  2.5194  37.8
## 312  0.79041  5.98  9.90 0.5440    18.4 6.122 304  2.6403  52.8
## 313  0.26169 11.72  9.90 0.5440    18.4 6.023 304  2.8340  90.4
## 314  0.26938  7.90  9.90 0.5440    18.4 6.266 304  3.2628  82.8
## 315  0.36920  9.28  9.90 0.5440    18.4 6.567 304  3.6023  87.3
## 316  0.25356 11.50  9.90 0.5440    18.4 5.705 304  3.9450  77.7
## 317  0.31827 18.33  9.90 0.5440    18.4 5.914 304  3.9986  83.2
## 318  0.24522 15.94  9.90 0.5440    18.4 5.782 304  4.0317  71.7
## 319  0.40202 10.36  9.90 0.5440    18.4 6.382 304  3.5325  67.2
## 320  0.47547 12.73  9.90 0.5440    18.4 6.113 304  4.0019  58.8
## 321  0.16760  7.20  7.38 0.4930    19.6 6.426 287  4.5404  52.3
## 322  0.18159  6.87  7.38 0.4930    19.6 6.376 287  4.5404  54.3
## 323  0.35114  7.70  7.38 0.4930    19.6 6.041 287  4.7211  49.9
## 324  0.28392 11.74  7.38 0.4930    19.6 5.708 287  4.7211  74.3
## 325  0.34109  6.12  7.38 0.4930    19.6 6.415 287  4.7211  40.1
## 326  0.19186  5.08  7.38 0.4930    19.6 6.431 287  5.4159  14.7
## 327  0.30347  6.15  7.38 0.4930    19.6 6.312 287  5.4159  28.9
## 328  0.24103 12.79  7.38 0.4930    19.6 6.083 287  5.4159  43.7
## 329  0.06617  9.97  3.24 0.4600    16.9 5.868 430  5.2146  25.8
## 330  0.06724  7.34  3.24 0.4600    16.9 6.333 430  5.2146  17.2
## 331  0.04544  9.09  3.24 0.4600    16.9 6.144 430  5.8736  32.2
## 332  0.05023 12.43  6.06 0.4379    16.9 5.706 304  6.6407  28.4
## 333  0.03466  7.83  6.06 0.4379    16.9 6.031 304  6.6407  23.3
## 334  0.05083  5.68  5.19 0.5150    20.2 6.316 224  6.4584  38.1
## 335  0.03738  6.75  5.19 0.5150    20.2 6.310 224  6.4584  38.5
## 336  0.03961  8.01  5.19 0.5150    20.2 6.037 224  5.9853  34.5
## 337  0.03427  9.80  5.19 0.5150    20.2 5.869 224  5.2311  46.3
## 338  0.03041 10.56  5.19 0.5150    20.2 5.895 224  5.6150  59.6
## 339  0.03306  8.51  5.19 0.5150    20.2 6.059 224  4.8122  37.3
## 340  0.05497  9.74  5.19 0.5150    20.2 5.985 224  4.8122  45.4
## 341  0.06151  9.29  5.19 0.5150    20.2 5.968 224  4.8122  58.5
## 342  0.01301  5.49  1.52 0.4420    15.5 7.241 284  7.0379  49.3
## 343  0.02498  8.65  1.89 0.5180    15.9 6.540 422  6.2669  59.7
## 344  0.02543  7.18  3.78 0.4840    17.6 6.696 370  5.7321  56.4
## 345  0.03049  4.61  3.78 0.4840    17.6 6.874 370  6.4654  28.1
## 346  0.03113 10.53  4.39 0.4420    18.8 6.014 352  8.0136  48.5
## 347  0.06162 12.67  4.39 0.4420    18.8 5.898 352  8.0136  52.3
## 348  0.01870  6.36  4.15 0.4290    17.9 6.516 351  8.5353  27.7
## 349  0.01501  5.99  2.01 0.4350    17.0 6.635 280  8.3440  29.7
## 350  0.02899  5.89  1.25 0.4290    19.7 6.939 335  8.7921  34.5
## 351  0.06211  5.98  1.25 0.4290    19.7 6.490 335  8.7921  44.4
## 352  0.07950  5.49  1.69 0.4110    18.3 6.579 411 10.7103  35.9
## 353  0.07244  7.79  1.69 0.4110    18.3 5.884 411 10.7103  18.5
## 354  0.01709  4.50  2.02 0.4100    17.0 6.728 187 12.1265  36.1
## 355  0.04301  8.05  1.91 0.4130    22.0 5.663 334 10.5857  21.9
## 356  0.10659  5.57  1.91 0.4130    22.0 5.936 334 10.5857  19.5
## 357  8.98296 17.60 18.10 0.7700    20.2 6.212 666  2.1222  97.4
## 358  3.84970 13.27 18.10 0.7700    20.2 6.395 666  2.5052  91.0
## 359  5.20177 11.48 18.10 0.7700    20.2 6.127 666  2.7227  83.4
## 360  4.26131 12.67 18.10 0.7700    20.2 6.112 666  2.5091  81.3
## 361  4.54192  7.79 18.10 0.7700    20.2 6.398 666  2.5182  88.0
## 362  3.83684 14.19 18.10 0.7700    20.2 6.251 666  2.2955  91.1
## 363  3.67822 10.19 18.10 0.7700    20.2 5.362 666  2.1036  96.2
## 364  4.22239 14.64 18.10 0.7700    20.2 5.803 666  1.9047  89.0
## 365  3.47428  5.29 18.10 0.7180    20.2 8.780 666  1.9047  82.9
## 366  4.55587  7.12 18.10 0.7180    20.2 3.561 666  1.6132  87.9
## 367  3.69695 14.00 18.10 0.7180    20.2 4.963 666  1.7523  91.4
## 368 13.52220 13.33 18.10 0.6310    20.2 3.863 666  1.5106 100.0
## 369  4.89822  3.26 18.10 0.6310    20.2 4.970 666  1.3325 100.0
## 370  5.66998  3.73 18.10 0.6310    20.2 6.683 666  1.3567  96.8
## 371  6.53876  2.96 18.10 0.6310    20.2 7.016 666  1.2024  97.5
## 372  9.23230  9.53 18.10 0.6310    20.2 6.216 666  1.1691 100.0
## 373  8.26725  8.88 18.10 0.6680    20.2 5.875 666  1.1296  89.6
## 374 11.10810 34.77 18.10 0.6680    20.2 4.906 666  1.1742 100.0
## 375 18.49820 37.97 18.10 0.6680    20.2 4.138 666  1.1370 100.0
## 376 19.60910 13.44 18.10 0.6710    20.2 7.313 666  1.3163  97.9
## 377 15.28800 23.24 18.10 0.6710    20.2 6.649 666  1.3449  93.3
## 378  9.82349 21.24 18.10 0.6710    20.2 6.794 666  1.3580  98.8
## 379 23.64820 23.69 18.10 0.6710    20.2 6.380 666  1.3861  96.2
## 380 17.86670 21.78 18.10 0.6710    20.2 6.223 666  1.3861 100.0
## 381 88.97620 17.21 18.10 0.6710    20.2 6.968 666  1.4165  91.9
## 382 15.87440 21.08 18.10 0.6710    20.2 6.545 666  1.5192  99.1
## 383  9.18702 23.60 18.10 0.7000    20.2 5.536 666  1.5804 100.0
## 384  7.99248 24.56 18.10 0.7000    20.2 5.520 666  1.5331 100.0
## 385 20.08490 30.63 18.10 0.7000    20.2 4.368 666  1.4395  91.2
## 386 16.81180 30.81 18.10 0.7000    20.2 5.277 666  1.4261  98.1
## 387 24.39380 28.28 18.10 0.7000    20.2 4.652 666  1.4672 100.0
## 388 22.59710 31.99 18.10 0.7000    20.2 5.000 666  1.5184  89.5
## 389 14.33370 30.62 18.10 0.7000    20.2 4.880 666  1.5895 100.0
## 390  8.15174 20.85 18.10 0.7000    20.2 5.390 666  1.7281  98.9
## 391  6.96215 17.11 18.10 0.7000    20.2 5.713 666  1.9265  97.0
## 392  5.29305 18.76 18.10 0.7000    20.2 6.051 666  2.1678  82.5
## 393 11.57790 25.68 18.10 0.7000    20.2 5.036 666  1.7700  97.0
## 394  8.64476 15.17 18.10 0.6930    20.2 6.193 666  1.7912  92.6
## 395 13.35980 16.35 18.10 0.6930    20.2 5.887 666  1.7821  94.7
## 396  8.71675 17.12 18.10 0.6930    20.2 6.471 666  1.7257  98.8
## 397  5.87205 19.37 18.10 0.6930    20.2 6.405 666  1.6768  96.0
## 398  7.67202 19.92 18.10 0.6930    20.2 5.747 666  1.6334  98.9
## 399 38.35180 30.59 18.10 0.6930    20.2 5.453 666  1.4896 100.0
## 400  9.91655 29.97 18.10 0.6930    20.2 5.852 666  1.5004  77.8
## 401 25.04610 26.77 18.10 0.6930    20.2 5.987 666  1.5888 100.0
## 402 14.23620 20.32 18.10 0.6930    20.2 6.343 666  1.5741 100.0
## 403  9.59571 20.31 18.10 0.6930    20.2 6.404 666  1.6390 100.0
## 404 24.80170 19.77 18.10 0.6930    20.2 5.349 666  1.7028  96.0
## 405 41.52920 27.38 18.10 0.6930    20.2 5.531 666  1.6074  85.4
## 406 67.92080 22.98 18.10 0.6930    20.2 5.683 666  1.4254 100.0
## 407 20.71620 23.34 18.10 0.6590    20.2 4.138 666  1.1781 100.0
## 408 11.95110 12.13 18.10 0.6590    20.2 5.608 666  1.2852 100.0
## 409  7.40389 26.40 18.10 0.5970    20.2 5.617 666  1.4547  97.9
## 410 14.43830 19.78 18.10 0.5970    20.2 6.852 666  1.4655 100.0
## 411 51.13580 10.11 18.10 0.5970    20.2 5.757 666  1.4130 100.0
## 412 14.05070 21.22 18.10 0.5970    20.2 6.657 666  1.5275 100.0
## 413 18.81100 34.37 18.10 0.5970    20.2 4.628 666  1.5539 100.0
## 414 28.65580 20.08 18.10 0.5970    20.2 5.155 666  1.5894 100.0
## 415 45.74610 36.98 18.10 0.6930    20.2 4.519 666  1.6582 100.0
## 416 18.08460 29.05 18.10 0.6790    20.2 6.434 666  1.8347 100.0
## 417 10.83420 25.79 18.10 0.6790    20.2 6.782 666  1.8195  90.8
## 418 25.94060 26.64 18.10 0.6790    20.2 5.304 666  1.6475  89.1
## 419 73.53410 20.62 18.10 0.6790    20.2 5.957 666  1.8026 100.0
## 420 11.81230 22.74 18.10 0.7180    20.2 6.824 666  1.7940  76.5
## 421 11.08740 15.02 18.10 0.7180    20.2 6.411 666  1.8589 100.0
## 422  7.02259 15.70 18.10 0.7180    20.2 6.006 666  1.8746  95.3
## 423 12.04820 14.10 18.10 0.6140    20.2 5.648 666  1.9512  87.6
## 424  7.05042 23.29 18.10 0.6140    20.2 6.103 666  2.0218  85.1
## 425  8.79212 17.16 18.10 0.5840    20.2 5.565 666  2.0635  70.6
## 426 15.86030 24.39 18.10 0.6790    20.2 5.896 666  1.9096  95.4
## 427 12.24720 15.69 18.10 0.5840    20.2 5.837 666  1.9976  59.7
## 428 37.66190 14.52 18.10 0.6790    20.2 6.202 666  1.8629  78.7
## 429  7.36711 21.52 18.10 0.6790    20.2 6.193 666  1.9356  78.1
## 430  9.33889 24.08 18.10 0.6790    20.2 6.380 666  1.9682  95.6
## 431  8.49213 17.64 18.10 0.5840    20.2 6.348 666  2.0527  86.1
## 432 10.06230 19.69 18.10 0.5840    20.2 6.833 666  2.0882  94.3
## 433  6.44405 12.03 18.10 0.5840    20.2 6.425 666  2.2004  74.8
## 434  5.58107 16.22 18.10 0.7130    20.2 6.436 666  2.3158  87.9
## 435 13.91340 15.17 18.10 0.7130    20.2 6.208 666  2.2222  95.0
## 436 11.16040 23.27 18.10 0.7400    20.2 6.629 666  2.1247  94.6
## 437 14.42080 18.05 18.10 0.7400    20.2 6.461 666  2.0026  93.3
## 438 15.17720 26.45 18.10 0.7400    20.2 6.152 666  1.9142 100.0
## 439 13.67810 34.02 18.10 0.7400    20.2 5.935 666  1.8206  87.9
## 440  9.39063 22.88 18.10 0.7400    20.2 5.627 666  1.8172  93.9
## 441 22.05110 22.11 18.10 0.7400    20.2 5.818 666  1.8662  92.4
## 442  9.72418 19.52 18.10 0.7400    20.2 6.406 666  2.0651  97.2
## 443  5.66637 16.59 18.10 0.7400    20.2 6.219 666  2.0048 100.0
## 444  9.96654 18.85 18.10 0.7400    20.2 6.485 666  1.9784 100.0
## 445 12.80230 23.79 18.10 0.7400    20.2 5.854 666  1.8956  96.6
## 446 10.67180 23.98 18.10 0.7400    20.2 6.459 666  1.9879  94.8
## 447  6.28807 17.79 18.10 0.7400    20.2 6.341 666  2.0720  96.4
## 448  9.92485 16.44 18.10 0.7400    20.2 6.251 666  2.1980  96.6
## 449  9.32909 18.13 18.10 0.7130    20.2 6.185 666  2.2616  98.7
## 450  7.52601 19.31 18.10 0.7130    20.2 6.417 666  2.1850  98.3
## 451  6.71772 17.44 18.10 0.7130    20.2 6.749 666  2.3236  92.6
## 452  5.44114 17.73 18.10 0.7130    20.2 6.655 666  2.3552  98.2
## 453  5.09017 17.27 18.10 0.7130    20.2 6.297 666  2.3682  91.8
## 454  8.24809 16.74 18.10 0.7130    20.2 7.393 666  2.4527  99.3
## 455  9.51363 18.71 18.10 0.7130    20.2 6.728 666  2.4961  94.1
## 456  4.75237 18.13 18.10 0.7130    20.2 6.525 666  2.4358  86.5
## 457  4.66883 19.01 18.10 0.7130    20.2 5.976 666  2.5806  87.9
## 458  8.20058 16.94 18.10 0.7130    20.2 5.936 666  2.7792  80.3
## 459  7.75223 16.23 18.10 0.7130    20.2 6.301 666  2.7831  83.7
## 460  6.80117 14.70 18.10 0.7130    20.2 6.081 666  2.7175  84.4
## 461  4.81213 16.42 18.10 0.7130    20.2 6.701 666  2.5975  90.0
## 462  3.69311 14.65 18.10 0.7130    20.2 6.376 666  2.5671  88.4
## 463  6.65492 13.99 18.10 0.7130    20.2 6.317 666  2.7344  83.0
## 464  5.82115 10.29 18.10 0.7130    20.2 6.513 666  2.8016  89.9
## 465  7.83932 13.22 18.10 0.6550    20.2 6.209 666  2.9634  65.4
## 466  3.16360 14.13 18.10 0.6550    20.2 5.759 666  3.0665  48.2
## 467  3.77498 17.15 18.10 0.6550    20.2 5.952 666  2.8715  84.7
## 468  4.42228 21.32 18.10 0.5840    20.2 6.003 666  2.5403  94.5
## 469 15.57570 18.13 18.10 0.5800    20.2 5.926 666  2.9084  71.0
## 470 13.07510 14.76 18.10 0.5800    20.2 5.713 666  2.8237  56.7
## 471  4.34879 16.29 18.10 0.5800    20.2 6.167 666  3.0334  84.0
## 472  4.03841 12.87 18.10 0.5320    20.2 6.229 666  3.0993  90.7
## 473  3.56868 14.36 18.10 0.5800    20.2 6.437 666  2.8965  75.0
## 474  4.64689 11.66 18.10 0.6140    20.2 6.980 666  2.5329  67.6
## 475  8.05579 18.14 18.10 0.5840    20.2 5.427 666  2.4298  95.4
## 476  6.39312 24.10 18.10 0.5840    20.2 6.162 666  2.2060  97.4
## 477  4.87141 18.68 18.10 0.6140    20.2 6.484 666  2.3053  93.6
## 478 15.02340 24.91 18.10 0.6140    20.2 5.304 666  2.1007  97.3
## 479 10.23300 18.03 18.10 0.6140    20.2 6.185 666  2.1705  96.7
## 480 14.33370 13.11 18.10 0.6140    20.2 6.229 666  1.9512  88.0
## 481  5.82401 10.74 18.10 0.5320    20.2 6.242 666  3.4242  64.7
## 482  5.70818  7.74 18.10 0.5320    20.2 6.750 666  3.3317  74.9
## 483  5.73116  7.01 18.10 0.5320    20.2 7.061 666  3.4106  77.0
## 484  2.81838 10.42 18.10 0.5320    20.2 5.762 666  4.0983  40.3
## 485  2.37857 13.34 18.10 0.5830    20.2 5.871 666  3.7240  41.9
## 486  3.67367 10.58 18.10 0.5830    20.2 6.312 666  3.9917  51.9
## 487  5.69175 14.98 18.10 0.5830    20.2 6.114 666  3.5459  79.8
## 488  4.83567 11.45 18.10 0.5830    20.2 5.905 666  3.1523  53.2
## 489  0.15086 18.06 27.74 0.6090    20.1 5.454 711  1.8209  92.7
## 490  0.18337 23.97 27.74 0.6090    20.1 5.414 711  1.7554  98.3
## 491  0.20746 29.68 27.74 0.6090    20.1 5.093 711  1.8226  98.0
## 492  0.10574 18.07 27.74 0.6090    20.1 5.983 711  1.8681  98.8
## 493  0.11132 13.35 27.74 0.6090    20.1 5.983 711  2.1099  83.5
## 494  0.17331 12.01  9.69 0.5850    19.2 5.707 391  2.3817  54.0
## 495  0.27957 13.59  9.69 0.5850    19.2 5.926 391  2.3817  42.6
## 496  0.17899 17.60  9.69 0.5850    19.2 5.670 391  2.7986  28.8
## 497  0.28960 21.14  9.69 0.5850    19.2 5.390 391  2.7986  72.9
## 498  0.26838 14.10  9.69 0.5850    19.2 5.794 391  2.8927  70.6
## 499  0.23912 12.92  9.69 0.5850    19.2 6.019 391  2.4091  65.3
## 500  0.17783 15.10  9.69 0.5850    19.2 5.569 391  2.3999  73.5
## 501  0.22438 14.33  9.69 0.5850    19.2 6.027 391  2.4982  79.7
## 502  0.06263  9.67 11.93 0.5730    21.0 6.593 273  2.4786  69.1
## 503  0.04527  9.08 11.93 0.5730    21.0 6.120 273  2.2875  76.7
## 504  0.06076  5.64 11.93 0.5730    21.0 6.976 273  2.1675  91.0
## 505  0.10959  6.48 11.93 0.5730    21.0 6.794 273  2.3889  89.3
## 506  0.04741  7.88 11.93 0.5730    21.0 6.030 273  2.5050  80.8</code></pre>
<pre class="r"><code>Boston %&gt;% 
  select(crim, lstat, indus, nox, ptratio, rm, tax, dis, age) %&gt;% 
  gather(obs, val, -crim) %&gt;% 
  ggplot(aes(val, crim, color=obs, fill=obs)) + 
  geom_smooth(method=&quot;lm&quot;, se=TRUE) +
  geom_point() +
  facet_wrap(~obs, scales=&quot;free_x&quot;) +
  scale_color_discrete(guide=FALSE) + 
  scale_fill_discrete(guide=FALSE)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<pre><code>## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = &quot;none&quot;` instead.</code></pre>
<pre><code>## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = &quot;none&quot;` instead.</code></pre>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>#  Data for 506 census tracts in Boston.
# medv (median house value)
# lstat (percent of households with low socioeconomic status)

# rm (average number of rooms per house),

# age (average age of houses)

glimpse(Boston)</code></pre>
<pre><code>## Rows: 506
## Columns: 14
## $ crim    &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,~
## $ zn      &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1~
## $ indus   &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.~
## $ chas    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ nox     &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,~
## $ rm      &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,~
## $ age     &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9~
## $ dis     &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505~
## $ rad     &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,~
## $ tax     &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31~
## $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15~
## $ black   &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90~
## $ lstat   &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10~
## $ medv    &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15~</code></pre>
<pre class="r"><code>str(Boston)</code></pre>
<pre><code>## &#39;data.frame&#39;:    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ black  : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...</code></pre>
<pre class="r"><code>lm(medv~lstat, Boston)%&gt;%
summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = medv ~ lstat, data = Boston)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.168  -3.990  -1.318   2.034  24.500 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***
## lstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.216 on 504 degrees of freedom
## Multiple R-squared:  0.5441, Adjusted R-squared:  0.5432 
## F-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Note el orden en la inclusión de los argumentos</strong> primero tenemos la variable dependiente, seguida por las variables independientes.</p>
</div>
<div id="formalización-del-modelo" class="section level4">
<h4>Formalización del modelo</h4>
<pre class="r"><code>library(tidyverse)
library(wooldridge)
library(ISLR)</code></pre>
<p>En términos matemáticos podemos describir la relación entre las variables del modelo con la expresión siguiente:</p>
<p><span class="math display" id="eq:align">\[\begin{align}
Y \approx \beta_0 + \beta_1 X
\tag{1}
\end{align}\]</span></p>
<p>Una vez que hemos estimado los parámetros del modelo podemos usarlos para hacer la predicción de <span class="math inline">\(\hat y\)</span></p>
<p><span class="math display" id="eq:mod">\[\begin{align}
\hat y= \hat \beta_0 + \hat \beta_1 x
\tag{2}
\end{align}\]</span></p>
<p><img src="/img/lm4.jpg" /></p>
<p>Note que la diferencia entre la linea de regresión y cada observación (lineas grises) da cuenta del error de predicción.</p>
<p><span class="math display">\[e_i=y_i - \hat y_i\]</span>
El modelo de regresión toma la suma al cuadrado de estos errores para determinar la estimación de la linea de mejor ajuste.</p>
<p>Suponemos una serie de axiomas que conforman el fundamento teórico de esta modelo a partir del <strong>Teorema de Gasuss Markov</strong>.</p>
<p>Suponemos un valor esperado para el término aleatorio (<em>random noise</em>) <span class="math inline">\(E(\epsilon)=0\)</span>. Esto es, suponemos que la distribución de estos factores no observados, en promedio tienen media cero.</p>
</div>
<div id="supuesto-de-media-condicional-cero." class="section level4">
<h4>Supuesto de Media condicional cero.</h4>
<p>Sean <span class="math inline">\(x\)</span> y <span class="math inline">\(\epsilon\)</span> variables aleatorias.</p>
<p>Suponemos que el valor promedio de las características no observadas no depende del valor particular que tome <span class="math inline">\(x\)</span> <span class="math display">\[E(\epsilon|x)=E(\epsilon)=0\]</span></p>
<p>El valor esperado de las características no observadas, es independiente de <span class="math inline">\(x\)</span> y en términos promedio es cero.</p>
<p>Definición:</p>
<p><em>Residual Sum of Squares</em> <strong>RSS</strong>.</p>
<p><span class="math display">\[RSS=e_1^2 +e_2^2+...+e_n^2\]</span>
<span class="math display" id="eq:RSS">\[\begin{align}
RSS= (y_1- \hat \beta_0 - \hat \beta_1 x_1)^2 + (y_2- \hat \beta_0 - \hat \beta_1 x_2)^2+...+(y_n- \hat \beta_0 - \hat \beta_1 x_n)^2
\tag{3}
\end{align}\]</span></p>
<p><img src="/img/rss1.jpg" />
El enfoque de Mínimos Cuadrados Ordinarios <strong>MCO</strong> (Ordinary Least Squares OLS por sus siglas en inglés), selecciona los parámetros <span class="math inline">\(\hat \beta_0 , \hat \beta_1\)</span> que minimizan la <strong>RSS</strong>.</p>
</div>
<div id="coeficientes-acorde-con-el-criterio-mco." class="section level4">
<h4>Coeficientes acorde con el criterio MCO.</h4>
<p><span class="math display" id="eq:beta">\[\begin{align}
\hat \beta_1 =\frac{ \sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n} (x_i-\bar{x})^2},
\tag{4}
\end{align}\]</span></p>
<p>El parámetro <span class="math inline">\(\hat \beta_1\)</span> es el cociente de la covarianza de <span class="math inline">\((x,y)\)</span> y la varianza de <span class="math inline">\(x\)</span>. Dividir por <span class="math inline">\(\frac{1}{n}\)</span> ambos factores no hace diferencia alguna.</p>
<p><span class="math display">\[\hat \beta_0 =\bar y- \hat \beta_1 \bar{x}\]</span>
Donde <span class="math display">\[\bar{y}\equiv\frac{1}{n} \sum_{i=1}^{n} y_i\]</span> y <span class="math display">\[\bar{x} \equiv \frac{1}{n} \sum_{i=1}^{n} x_i\]</span></p>
<p>Note por la definición que si <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> tienen correlación positiva, <span class="math inline">\(\Rightarrow \hat \beta_1&gt;0\)</span> y si <span class="math inline">\(\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})&lt;0 \Rightarrow \hat \beta_1&lt;0\)</span>
dado que <span class="math inline">\(\sigma^2\)</span> tiene un min val=0, ie. <span class="math inline">\(\sigma^2\)</span> nunca es negativo$</p>
<p>Considere la siguiente representación del modelo lineal a partir del valor esperado y con los supuestos <span class="math inline">\(E(\epsilon)=0\)</span> e independencia entre la parte aleatoria del modelo y las variables explicativas: y <span class="math inline">\(cov(x,\epsilon)=E(x \epsilon)=0\)</span></p>
<p><span class="math display" id="eq:ve">\[\begin{align}
E(y-\beta_0-\beta_1 x)=0
\tag{5}
\end{align}\]</span></p>
<p><span class="math display" id="eq:ve1">\[\begin{align}
E [x(y-\beta_0-\beta_1 x)]=0
\tag{6}
\end{align}\]</span></p>
<p>Los parámetros muestrales correspondientes</p>
<p><span class="math display" id="eq:ve3">\[\begin{align}
\frac{1}{n}\sum_{i=1}^{n} (y_i-\hat \beta_0- \hat \beta_1 x_i) =0
\tag{7}
\end{align}\]</span></p>
<p><span class="math display" id="eq:ve4">\[\begin{align}
\frac{1}{n} \sum_{i=1}^{n} x_i(y_i-\hat \beta_0- \hat \beta_1 x_i)=0
\tag{8}
\end{align}\]</span></p>
<p>Ecuaciones <a href="#eq:ve3">(7)</a> y <a href="#eq:ve4">(8)</a> son las condiciones de primer orden a resolver acorde con los principios de optimización.</p>
<p>A partir de las propiedades de suma estas ecuaciones se pueden reescribir:</p>
<p><span class="math display" id="eq:ve5">\[\begin{align}
\bar{y}=\hat \beta_0- \hat \beta_1 \bar{x}
\tag{9}
\end{align}\]</span></p>
<p>Reescribiendo <span class="math inline">\(\hat \beta_0\)</span> y <span class="math inline">\(\hat \beta_1\)</span> en términos de <span class="math inline">\(\bar{y}\)</span> y <span class="math inline">\(\bar{x}\)</span> tenemos:</p>
<p><span class="math display" id="eq:ve6">\[\begin{align}
\hat \beta_0= \bar{y} - \hat \beta_1 \bar {x}=0
\tag{10}
\end{align}\]</span></p>
<p>Sustituyendo <a href="#eq:ve6">(10)</a> en <a href="#eq:ve4">(8)</a> tenemos</p>
<p><span class="math display">\[\sum_{i=1}^{n} x_i [ y_i-(\bar y- \beta_1 \bar{x})- \hat \beta_1 x_i]=0\]</span></p>
<p>Reordenando tenemos:</p>
<p><span class="math display">\[\sum_{i=1}^{n}x_i(y_i-\bar{y})= \hat \beta_1  \sum_{i=1}^{n} x_i(x_i-\bar {x})\]</span></p>
<p>Considerando propiedades de suma tenemos:</p>
<p><span class="math display">\[\sum_{i=1}^{n} x_i(y_i- \bar{y})= \sum_{i=1}^{n} (x_i- \bar{x})(y_i- \bar{y}) , \sum_{i=1}^{n} x_i(x_i- \bar{x})=\sum_{i=1}^{n} (x_i- \bar{x})^2\]</span>
Si <span class="math inline">\(\sum_{i=1}^{n} (x_i- \bar{x})^2&gt;0\)</span> lo cual se cumple cuando <strong>NO</strong> todos los valores de la muestra son iguales, entonces:</p>
<p><span class="math display" id="eq:ve7">\[\begin{align}
\hat \beta_1=  \frac{\sum_{i=1}^{n} (x_i- \bar{x})(y_i- \bar{y}) }{\sum_{i=1}^{n} (x_i- \bar{x})^2}
\tag{11}
\end{align}\]</span></p>
<p>Considerando el <strong>supuesto de Media condicional cero</strong> <span class="math inline">\(E(\epsilon|x)=0\)</span> y representando el valor esperado de <a href="#eq:mod">(2)</a> condicional en <span class="math inline">\(x\)</span> tenemos una función lineal de <span class="math inline">\(x\)</span>:</p>
<p><span class="math display" id="eq:expval">\[\begin{align}
E(y|x)= \beta_0+ \beta_1 X
\tag{12}
\end{align}\]</span></p>
<p>La siguiente gráfica muestra esta idea para la función lineal <span class="math inline">\(\beta_0+\beta_1 x\)</span> con la distribución de los valores de <span class="math inline">\(y\)</span> centrados en <span class="math inline">\(E(y|x)\)</span></p>
<p><img src="/img/lm6.jpg" width="400" /></p>
<p>Lo que significa que <strong>un incremento de una unidad en <span class="math inline">\(x\)</span> cambia el valor esperado de <span class="math inline">\(y\)</span> por el monto denotado <span class="math inline">\(\hat \beta_1\)</span></strong>, para cualquier valor dado de <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> que está centrado alrededor de <span class="math inline">\(E(y|x)\)</span>.</p>
<p>En términos de la ecuación lineal el estimado de la pendiente es:
<span class="math display">\[\hat \beta_1= \frac{\Delta \hat y}{\Delta  x}\]</span>
<span class="math display">\[\Delta \hat y=\hat \beta_1 \Delta  x\]</span></p>
<p>En la siguiente figura se presenta ésta idea con una muestra teórica de valores aleatorios para <span class="math inline">\(x\)</span> y <span class="math inline">\(y\)</span>.</p>
<p>El panel a) muestra en linea roja la verdadera relación en entre <span class="math inline">\(x\)</span> y <span class="math inline">\(y\)</span>, la linea azul es la linea de regresión obtenidad por MCO que se basa en los datos observados (scatter points).</p>
<p>En panel b) se incluyen las lineas de regresión para 10 modelos con muestras aleatorias en la población de <span class="math inline">\(x\)</span>, vemos que cada modelo permite generar una línea distinta si bien, en <em>promedio</em> estas siguen la trayectoria de la linea de regresión poblacional (roja).</p>
<p>En el panel c) Observamos el efecto del criterio de minimos cuadrados con la obtensión del <em>RSS</em> de menor valor para los parámetros <span class="math inline">\(\hat \beta_0\)</span> y <span class="math inline">\(\hat \beta_1\)</span></p>
<p><img src="/img/lm5.jpg" /></p>
<p><strong>Ejemplo 1</strong></p>
<p>Considerando el ejemplo de la relación entre salario y años de educación <span class="math inline">\(log(wage)=\beta_0 +\beta_1 educ+ \epsilon\)</span></p>
<p>Con el logaritmo de salario (<strong>wage</strong> en dólares por hora) para medir el cambio porcentual ante el incremento de un año adicional de educación con la variable <strong>educ</strong> que se registra en años.</p>
<p>Recordamos que <span class="math inline">\(\epsilon\)</span> integra un conjunto de variables que se asumen constantes como experiencia, habilidades, etc., Y estas tienen una distribución representada por el valor esperado normalizado en cero <span class="math inline">\(E(\epsilon)=0\)</span>. (algunas personas tienen más habilidad natural que otras, más o menos experiencia, considerar el supuesto de la distribución con media cero es consistente con este hecho empírico).</p>
<p>La gráfica siguiente muestra la linea de regresión (rojo) para las variables salarios y educación con las observaciones (azul) como un scatter plot.</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Notamos una relación positiva y a medida que la persona tiene más años de educación en promedio percibe un mayor salario.</p>
<p>Tenemos una linea de regresión obtenida bajo el criterio de MCO y observamos una distancia entre la linea de regresión y las observaciones, sin embargo sabemos que la linea de regresión minimiza esa distancia.</p>
<p>Considerando los parámetros estimados para este ejemplo sobre la relación salario y años de educación tenemos que <span class="math inline">\(\hat \beta_0=0.583773\)</span> y <span class="math inline">\(\hat \beta_1=0.082744\)</span></p>
<p>Lo que implica que un año adicional de educación se relaciona con un incremento del 8.2% en el salario por hora percibido.</p>
<p><strong>Nota</strong> sobre el uso de logaritmos en la forma funcional del modelo de regresión lineal.</p>
<p><img src="/img/logs.jpg" /></p>
<pre><code>## 
## Call:
## lm(formula = lwage ~ educ, data = wage1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.21158 -0.36393 -0.07263  0.29712  1.52339 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.583773   0.097336   5.998 3.74e-09 ***
## educ        0.082744   0.007567  10.935  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4801 on 524 degrees of freedom
## Multiple R-squared:  0.1858, Adjusted R-squared:  0.1843 
## F-statistic: 119.6 on 1 and 524 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="ejemplo-2" class="section level4">
<h4>Ejemplo 2</h4>
<p>Consideremos ahora la base de datos: <em>Height and weight of school children</em> de Lewis, T., &amp; Taylor, L.R. (1967) en el estudio introduction to Experimental Ecology, que contiene 236 observaciones para 5 variables en una muestra de niños en edades entre 11 A 17 años, para estudiar la relación entre dos variables la altura y la edad. En términos teóricos se ha demostrado que entre la población munidal se tiene una estrecha relación entre estas variables.</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = heightIn ~ ageYear, data = heightweight)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3517 -1.9006  0.1378  1.9071  8.3371 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  37.4356     1.8281   20.48   &lt;2e-16 ***
## ageYear       1.7483     0.1329   13.15   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.989 on 234 degrees of freedom
## Multiple R-squared:  0.4249, Adjusted R-squared:  0.4225 
## F-statistic: 172.9 on 1 and 234 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## 
## Call:
## lm(formula = heightIn ~ ageYear, data = M)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.2725 -2.1208  0.4331  1.7637  6.4176 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  43.9634     2.4625  17.854  &lt; 2e-16 ***
## ageYear       1.2089     0.1787   6.767 6.88e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.831 on 109 degrees of freedom
## Multiple R-squared:  0.2958, Adjusted R-squared:  0.2893 
## F-statistic: 45.79 on 1 and 109 DF,  p-value: 6.875e-10</code></pre>
<pre><code>## 
## Call:
## lm(formula = heightIn ~ ageYear, data = H)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3444 -1.7206 -0.3108  1.4369  7.9299 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  30.6580     2.3420   13.09   &lt;2e-16 ***
## ageYear       2.3009     0.1707   13.48   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.721 on 123 degrees of freedom
## Multiple R-squared:  0.5964, Adjusted R-squared:  0.5931 
## F-statistic: 181.8 on 1 and 123 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Notamos un efecto positivo, que es de mayor magnitud para los niños. Con un incremento de <span class="math inline">\(\hat \beta_1= 2.03\)</span> pulgadas por un año adicional. Mientras el incremento para el sexo femenino es ligeramente inferior. <span class="math inline">\(\hat \beta_1=1.2\)</span></p>
<p>Note además que el modelo se estima con las varaibles en niveles por lo que la interpretación se hace considerando las unidades de cada variable en este caso edad en años y estatura en pulgadas.</p>
</div>
<div id="ejemplo-3" class="section level4">
<h4>Ejemplo 3</h4>
<p>Considermos ahora una situación en la que la variable dependiente es ventas (monto expresado en unidad monetaria ej. miles de dólares). Y las variables independientes corresponden a gastos en publicidad para tres medios: t.v., newspapers, radio. La muestra contiene 200 observaciones para las variables: Y=ventas, x=gasto en publicidad dirigido a tv. y corresponden a 200 puntos de venta.</p>
<p>El modelo de regresión siguiente muestra la relación entre la variable ventas y los gastos en publicidad para el medio t.v. (gasto expresado en miles de USD y y ventas en unidades).</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre><code>## 
## Call:
## lm(formula = sales ~ TV, data = ad)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3860 -1.9545 -0.1913  2.0671  7.2124 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***
## TV          0.047537   0.002691   17.67   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.259 on 198 degrees of freedom
## Multiple R-squared:  0.6119, Adjusted R-squared:  0.6099 
## F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Los parámetros estimados del modelo indican que un incremento de una unidad adicional (en este caso un incremento de <span class="math inline">\(\$ 1000\)</span>) en la variable gasto en publicidad en TV se asocia con un incremento de 47 unidades en la variable ventas <span class="math inline">\(\hat \beta_1=0.047537\)</span></p>
</div>
<div id="ejemplo-4" class="section level4">
<h4>Ejemplo 4</h4>
<p>Consideremos ahora la relación entre el Sueldo que perciben los CEO’s (Chief Executive Officer) y el indicador de desempeño financiero <em>return on equity</em> <strong>roe</strong> este es un indicador clásico que es utilizado como <em>proxy</em> para medir el desempeño de un CEO. La muestra contiene 209 observaciones y 12 variables con información financiera publicadas por un estudio de <em>Businessweek</em> de Mayo 6, 1991.</p>
<p>La variable dependiente es el Sueldo (expresado en miles USD). En términos teóricos se supone que existe una relación positiva entre el Sueldo que un CEO percibe e indicadores de desempeño de la compañia, en este sentido el <strong>roe</strong> es una variable que recoge el desempeño financiero (variable que se encuentra expresada en porcentaje).</p>
<p>Estimaremos ahora el modelo de regresión lineal <span class="math display">\[y=\hat \beta_0 + \hat \beta_1 x+\epsilon\]</span> <span class="math display">\[salario=\beta_0 + \hat \beta_1 roe+\epsilon\]</span> para conocer si la hipótesis sobre la relación entre las variables se cumple y en su caso la magnitud de los parámetros <span class="math inline">\(\hat \beta_0, \hat \beta_1\)</span></p>
<pre><code>## &#39;data.frame&#39;:    209 obs. of  12 variables:
##  $ salary  : int  1095 1001 1122 578 1368 1145 1078 1094 1237 833 ...
##  $ pcsalary: int  20 32 9 -9 7 5 10 7 16 5 ...
##  $ sales   : num  27595 9958 6126 16246 21783 ...
##  $ roe     : num  14.1 10.9 23.5 5.9 13.8 ...
##  $ pcroe   : num  106.4 -30.6 -16.3 -25.7 -3 ...
##  $ ros     : int  191 13 14 -21 56 55 62 44 37 37 ...
##  $ indus   : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ finance : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ consprod: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ utility : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ lsalary : num  7 6.91 7.02 6.36 7.22 ...
##  $ lsales  : num  10.23 9.21 8.72 9.7 9.99 ...
##  - attr(*, &quot;time.stamp&quot;)= chr &quot;25 Jun 2011 23:03&quot;</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     223     736    1039    1281    1407   14822</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.50   12.40   15.50   17.18   20.00   56.30</code></pre>
<pre><code>## 
## Call:
## lm(formula = salary ~ roe, data = ceosal1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1160.2  -526.0  -254.0   138.8 13499.9 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   963.19     213.24   4.517 1.05e-05 ***
## roe            18.50      11.12   1.663   0.0978 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1367 on 207 degrees of freedom
## Multiple R-squared:  0.01319,    Adjusted R-squared:  0.008421 
## F-statistic: 2.767 on 1 and 207 DF,  p-value: 0.09777</code></pre>
<p>Como es costrumbre en primera instancia exploramos la estructura de los datos. Tenemos 209 observaciones, 12 variables.</p>
<p>En segundo lugar analizamos las estadísticas descriptivas.
Para la variable Sueldo tenemos una media de <span class="math inline">\(\$1,281,000\)</span> usd anuales, un mínimo de <span class="math inline">\(\$223,000\)</span> y un máximo de <span class="math inline">\(\$14,822,000\)</span>. El rendimiento del capital promedio es <span class="math inline">\(17.18\%\)</span>, máximo de <span class="math inline">\(56.30\%\)</span> y mínimo de <span class="math inline">\(.50\%\)</span></p>
<p>En tercer lugar note por el parámetro <span class="math inline">\(\hat \beta_1\)</span> que hay una relación positiva ente Sueldo del CEO y el roe de la compañia, con <span class="math inline">\(\hat \beta_0=963.19\)</span> lo que implica que si el ROE es cero, la predicción del sueldo es <span class="math inline">\(\$963,190\)</span> y la pendiente <span class="math inline">\(\hat \beta_1=18.50\)</span>. Así <span class="math inline">\(\Delta x=1\%\Rightarrow \Delta y=\$ 18,500\)</span> En otras palabras ante <strong>un incremento de 1% en el roe</strong>, en promedio, el sueldo del CEO <span class="math inline">\(i\)</span> cambia en <span class="math inline">\(\$ 18,500\)</span> usd anuales.</p>
<p>La siguiente gráfica muestra estas relaciones distinguiendo por dos escenarios de interés (con variables categóricas)</p>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     223     736    1039    1281    1407   14822</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
</div>
</div>
<div id="actividad-3" class="section level2">
<h2>Actividad 3</h2>
<div id="ejercicio-1" class="section level4">
<h4>Ejercicio 1</h4>
<p>Considere el siguiente escenario sobre procesos electorales. Sea una <a href="https://drive.google.com/file/d/1lzbPJYbvk2C7uRk8JMB0RT3tvpoTl4X8/view?usp=sharing">muestra</a> de 173 resultados de votaciones distritales en un proceso entre dos partidos. La variable dependiente es el porcentaje de voto que recibió el candidato en la elección y la variable independiente es la proporción del gasto total de campaña correspondiente al candidato. Estudio de M. Barone and G. Ujifusa, The Almanac of American Politics, 1992.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Estime el modelo de regresión y determine <span class="math inline">\(\hat \beta_0, \hat \beta_1\)</span>.</p></li>
<li><p>¿Cuál es el efecto en el porcentaje del voto ante un incremento de <span class="math inline">\(1\%\)</span> en los gastos de campaña del candidato.</p></li>
<li><p>Estime el porcentaje del voto recibido si la proporción en gasto de campaña es de <span class="math inline">\(62\%\)</span> <span class="math inline">\(\Delta \hat voto= \hat \beta_1 \Delta gasto\)</span></p></li>
</ol>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/Unidad%204_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="actividad-para-participación-en-clase" class="section level4">
<h4>Actividad para participación en clase</h4>
<p>¿Cómo interpretamos una modelo de regresión cuya variable dependiente se expresa mediante una transformación logarítmica?</p>
<p>¿Qué interpretación tiene el coeficiente <span class="math inline">\(\hat \beta_1\)</span> en el modelo en logarítmos para las variables dependiente y explicativa.</p>
</div>
<div id="recomendación-de-bibliografía-de-estudio-en-casa-para-el-tema." class="section level4">
<h4>Recomendación de bibliografía de estudio en casa para el tema.</h4>
<p>Leer sección <em>The Effects of Changing Units of Measurement on OLS Statistics</em> y sección: <em>Incorporating Nonlinearities in Simple Regression</em> en Wooldridge (pags. 40-44)</p>
</div>
<div id="términos-clave" class="section level3">
<h3>Términos clave</h3>
<ul>
<li><p>Coeficiente de la pendiente <span class="math inline">\(\hat \beta_1\)</span></p></li>
<li><p>coeficiente de intercepto <span class="math inline">\(\hat \beta_0\)</span></p></li>
</ul>
</div>
<div id="términos-clave-1" class="section level3">
<h3>Términos clave</h3>
<p><strong>Least squares:</strong> Noción atribuida a Gauss. Criterio para la estimación del modelo de regresión lineal.</p>
</div>
<div id="referencias" class="section level3">
<h3>Referencias</h3>
<p>Box, G. E. P. (1979). Robustness in the strategy of scientific model building. In R. Launer &amp; G. Wilkinson (Eds.), Robustness in statistics (pp. 201–235). New York, NY: Academic Press.</p>
</div>
</div>
