---
title: "Tema: Medidas de tendencia central y dispersión"
author: "JLMR"
date: 2022-09-04T22:13:14-05:00
output: html_document
---



```{r, message=FALSE,warning=FALSE,include=FALSE}
library(tidyverse)
library(datagovindia)
library(plotly)
```



```{r set-global-options, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE, 
                      cache = FALSE,
                      include = TRUE,
                      collapse = FALSE,
                      dependson = NULL,
                      engine = "R", # Chunks will always have R code, unless noted
                      error = FALSE)                     

```



```{r silent-packages, echo = FALSE, eval = TRUE, message=FALSE, include = FALSE}

library(ggrepel)
library(tidyverse)
library(scales)
library(ggpubr)
library(plotly)
library(foreign)
library(datagovindia)
```

## Objetivos: 

Estudiar las medidas de tendencia central y dispersión.    

### Meta de aprendizaje: 
Implementar la estimación y representar visualmente las estimaciones usando R, Python. 

#### Contexo:  
Estadística descriptiva.


#### Definiciones.


Una pregunta básica para entender un fenómeno a escala regional es 

¿Qué tan frecuente ocurre un suceso? 

Por ejemplo podemos estar interesados en comprender la distribución del ingreso.

¿ Cuál es el ingreso promedio en México?

Estas y otras preguntas requieren del análisis de datos  y las medidas de tendencia central nos ayudan a obtener esta información.

### Media

El dato representativo, el promedio. 

**Notación:**

Media muestral:

$$\bar{X}=\frac{1}{n} (\sum_{i=1}^{n} x_i) =\frac{x_1 +x_2+...+x_n}{n}$$
Media poblacional con la letra griega:
$$\mu$$


Ejemplo  1

```{r}

# Defining vector
x <- c(3, 7, 5, 13, 20, 23, 39, 23, 40, 23, 14, 12, 56, 23)

# Print mean
print(mean(x))
```

### Media geométrica.

La media geométrica de una serie que contiene **n** elementos esta dada por:  

La raíz $n$ del producto de los elementos.

![](/img/geomed.jpg)
![](/img/geometrica.jpg)


Su aplicación se dirige a datos que tienen una naturaleza de 
**crecimiento exponencial** por ejemplo que
exhiben correlación serial, (dato en momento del tiempo t  que depende del dato en el momento *t-1*)
esto es por ejemplo común en el 
campo series de tiempo en economía y el area de las finanzas. Ejemplo, la tasa de crecimiento del PIB, la inflación, etc., 

Este tipo de variables no tiene un comportamiento aleatorio, pero sus observaciones son dependientes de los valores que tome la variable en momentos previos (lags.). 

**NOTE:** 

+ El valor de la media geométrica es siempre menor que la media aritmética.

+ Unicamente para valores positivos.
Dado el algoritmo de estimación, en los números reales no
esta definida la raíz  para 
números negativos. 

##### Ejemplo 1

Estime la media aritmética y geométrica para el ingreso percápita de los países para el año 1952 y 2007.  Base de datos de ingreso percapita mundial. 


```{r}

ingreso <- (data.frame(y=c(3, 7, 5, 13, 20, 23, 39, 23, 40, 23, 14, 12, 56, 23,"nd")))

nums <- as.numeric(as.character(ingreso$y))

m <- mean(nums, na.rm = T)


## SOLUCION.

## Paso 1 Cargamos las datos 

library(gapminder)
datos<-gapminder 

## Paso 2  vemos los datos, la estructura los nombres de variables 

view(datos)
names(datos)
glimpse(datos)

## paso 3  filtramos el año requerido

media_1952<-datos%>%filter(year=="1952")

## Paso 4  hacemos el cálculo usando la función summarise y mean. 

m_52<-summarise(media_1952,m=mean(gdpPercap))

```

```{r, include=FALSE}
## SOLUCION para año 2007.

media_2007<-datos%>%filter(year=="2007")
m_07<-summarise(media_2007,m=mean(gdpPercap))

## SOLUCIÓN MEDIA GEOMÉTRICA.


# Paso 1 Cargamos el paquete psych. este contiene la función para estimar la media geométrica.

library(psych)

# Paso 2 Procedimiento manual, calculamos el logaritmo, luego la media aritmética de esos valores y finalmente convertimos de logaritmo a niveles. (elevamos  la base "e" a la potencia indicada).

## Método paso a paso. 

## Creo columna con logaritmos de x y estimo la media para luego estimar el antilogaritmo. con "exp".

datosejemplo<-mutate(media_1952, logs=log(gdpPercap),medialogs=mean(logs), m52=exp(medialogs))

## Método resumido.

exp(mean(log(media_1952$gdpPercap)))

## Método directo con la función geometric.mean

geometric.mean(media_1952$gdpPercap)

```

Realice la estimación para 2007.

¿Qué podemos decir de la variación en el nivel de ingreso entre los países de la muestra en el periodo de **1952** a **2007**. ¿Cuál es la tasa de crecimiento?


##### Ejercicio 1 Estime la media aritmética y geométrica para las variables población y esperanza de vida por continente.


```{r, include=FALSE}
library(psych)

#geometric.mean(exa$ingreso.x)

#media_geo<-exp(mean(log(exa$ingreso.x)))

#media_geo

## Note que, en primera instancia estimamos el logaritmo de la variable  ingreso. La estimación del logaritmo permite tomar en consideración la naturaleza de crecimeinto exponencial que tiene este variable en particular.
## Tal como el algoritmo de estimación de media geométrica indica, obtenemos la media aritmética de los logaritmos y posteriormente reexpresamos el indicador a niveles, es decir no expresados en logaritmos.

#vNote que el cálculo usa la base de logaritmo natural que es tomando como base el número e=2.718281828....```
```

Para estimación en python See **Sesión2_tendencia central.ipynb**


### Mediana.

El valor que se localiza en la mitad de los datos cuando los datos se ordenan (ej. ascendente).

En contraste con la media, **la mediana** no es afectada por los puntos extremos.

La **mediana** es representativa  de la tendencia central cuando los datos no tienen una distribución simétrica. (cuando los datos tienen sesgo) Una expresión del sesgo es cuando la media y la mediana no son iguales. 

En este caso es  preferible la **mediana** como indicador representativo de tendencia dentral.
 
 
 Ejemplos de distribución con sesgo. 
 
![](/img/normal_.jpg)
 
*Improve your skills*[visualization training material](https://r-graphics.org/recipe-distribution-basic-density)
 
#### Ejemplo 3

```{r}
 # Defining vector
x <- c(3, 7, 5, 13, 20, 23, 39,
	23, 40, 23, 14, 12, 56, 23)

# Print Median
median(x)
```




## Moda

El valor que ocurre con mayor frecuencia en la distribución.

```{r}

# Defining vector
x <- c(3, 7, 5, 13, 20,  39, 23, 40,
	23, 14, 12, 56, 23, 29, 56, 37,
	45, 1, 25, 8, 56, 56)

# Generate frequency table
y <- table(x)

# Print frequency table
print(y)

# Mode of x

moda <- names(y)[which(y == max(y))]

# Print mode
print(moda)
```

Implementación en **R**


Dado un conjunto de datos cualquiera, podemos determinar las medidas de tendencia central aplicando la función (verbo) **summary** sobre el objeto (data frame deseado  -sustantivo-).



Ver Práctica de implementación en **Sesión2_tendencia central.ipynb**



##### Ejemplo 1 

**Estimación de medidas de tendencia central básicas.** 

**Datos:** Correlación entre Salario medio y participación de la Mujer por categoría de ocupación.


Buscamos deteminar el comportamiento del salario percibido entre un grupo de egresados universitarios de diversas disciplinas.

La base de datos contiene diversas características de la población de estudio incluido el salario (una varible numérica ordinal) y la variable que describe la disciplina en la que trabaja el individuo. 


La meta es estimar los salarios promedio para cada grupo ocupacional, así como representar graficamente el comparativo.

```{r, include=FALSE}

library(datagovindia)

url<-"https://github.com/bordercode/STATISTICS-I-MDR/blob/main/content/english/blog/graduados.rds?raw=true"

ejercicio_1<-read_rds_from_github(url)

```

```{r, message = FALSE, warning = FALSE, fig.width=10, fig.height=6, echo= TRUE}

## Remember to have loaded library(datagovindia)

# Cargamos los datos.
# get data using code chunk above.

## from url

url_grad<-("https://github.com/JoseLuisManzanaresRivera/intersemestral-2021/blob/main/content/post/graduados.rds?raw=true")

graduados<-read_rds_from_github(url_grad)


## Local  load 



# Exploración básica para conocer la estructura.

glimpse(graduados)

# Tipo de estructura de datos: Corte transversal (un momento en el tiempo).

# Con 64 observaciones. 21 variables.  Este formato de observacion por renglon y variable por columna lo denominaresmo -tidy data- o datos organizados con estructura estándar.

# Visualización  Scatter con trend para  4 features.  X= Proporcion mujeres/hombres por categoria, Y=mediana del salario, Categoria en color y otra Categoria en point size.

## Observamos variables numéricas, (denotadas int o dbl según sean numeros enteros o decimales), categóricas (denotadas fct).  Otro tipo de datos que no esta presente paro es comun es chr, (string de texto, ejemplo nombre de países, calles, etc.)


# Alternativa  para conocer la estructura de datos. Desventaja si son muchas variables es una gran tabla. 

str(graduados)

# Medidas de tendencia central y estadistica descriptiva básica.

summary(graduados)


 g<-ggplot(graduados,aes(ShareWomen,Median, color=Area,size=Sample_size, label=Major))+
  geom_point()+
  geom_smooth(aes(group=1),method = "lm")+
  expand_limits(y=0)+
  scale_color_brewer(palette="Dark2")+
  scale_y_continuous(labels=dollar_format())+
  scale_x_continuous(labels=percent_format())+
  labs(x="Mujeres/Hombres", y="Salario Medio  US$")+
    theme(legend.position="botton")+
   theme_light()
  
   ggplotly(g)

```


**Nota**: Como parte del proceso de los datos no solo estimamos la media  pero desplegamos una herramienta de viasualización denominado **Diagrama de Dispersión**  (representación gráfica) **bi-variada** 

Con la libreria ggplot de R podemos también representar este conjunto de datos  mediante el  diagrama de dispersión incorporando más dimensiones (no solo el salario y la proporción entre mujeres y hombres), pero también la categoria a la que corresponde la ocupación y el tamaño de la muestra, Lo anterior permite extraer una gran cantidad de información para perfilar alguna explicación o hipótesis sobre el fenómeno.

Estamos describiendo los datos, estamos haciendo análisis exploratorio de datos!

## Medidas de Dispersón: 

## Percentiles

Una pregunta muy frecuente cuando analizamos la distribución de los datos es ¿Qué porcentaje de los datos se encuentra por debajo de cierto umbral? 

Por ejemplo si la base contiene ingresos percapita nos podemos preguntar ¿Cuál es el nivel de ingreso por debajo del cuál se ubica el 60% de la población? 

Si estimamos el percentil 60, tenemos la respuesta a esta pregunta.

La estimación de los percentiles es informativa del grado de concentración de los datos hacia un valor particular. 

Por ejemplo, si estamos analizando la distribución de ingreso, nos puede interesar determinar que porcentaje de los datos presentan un ingreso igual o inferior a la linea de pobreza (este umbral es un dato particular *x*  de la variabe **X**, ej en Mexico **$13,133.30**).


#### Ejemplo 1 

Estimación de los percentiles p25, p50, p75. Estos percentiles en  particular corresponden con los cuantiles que de hecho dividen los datos en 4 partes Q1=.25, Q2=.50, Q3=.75, Q4=1.

```{r}
### Generamos una muestra aleatoria de 1000 observaciones para simular ingresos en un rango de 5000 a 60000 mensuales.  Note usamos función "runif". otras funciones  utiles para generar numeros aleatorios son r.norm().

ingresos<-data.frame(pesos=runif(1000, min=5000, max=60000))

# Paso 2. Aplicamos la función quantile para la variable de interes, note que la función admite dos argumentos, el primero, el objeto x y el segundo denominado probs. para definir el percentil de interés. Por default la función estima Q0  a Q4.
 
quantile(ingresos$pesos)
```


#### Ejercicio  1. 

Determine los cuantiles Q1, Q2, Q3, Q4 para la variable esperanza de vida (lifexp) expresada en años para la informacón por continente.

¿Qué continente tiene la mayor esperanza de vida considerando al menos el 75% de los datos disponibles.  (ej. ¿Qué continente tiene el mayor  **Q3**)?.

¿Cuál es la diferencia  entre el **Q1** de **Africa** y **Q1** del continente con mayor esperanza de vida?


```{r }

library(gapminder)

cuantiles<-gapminder

Africa<-cuantiles%>%filter(continent=="Africa")

quantile(Africa$lifeExp)  

### Interpretación. 

#Q0 mide el dato de menor valor de la base. 

#Q1 Indica que el 25% de los datos tiene una lifeExp de 42.3725 años o menos. 

#Q2 Indica  que  el 50% de la pobalción tiene un 
#lifeExp de 47.7920 o menos años.

#Q3 Indica  que  el 75% de la pobalción tiene un 
#lifeExp de 54.4115 o menos años.

#Q Indica  que  la mayor lifeExp en los paises de la muestra (Africa) es 75.4420 años.

Americas<-cuantiles%>%filter(continent=="Americas")
quantile(Americas$lifeExp)  

Asia<-cuantiles%>%filter(continent=="Asia")
quantile(Asia$lifeExp)  
 
Europa<-cuantiles%>%filter(continent=="Europe")
quantile(Europa$lifeExp)

Oceania<-cuantiles%>%filter(continent=="Oceania")
quantile(Oceania$lifeExp)

```


```{r}

## Estimación alternativa usando tidyverse aproach. 


## Paso 1 definimos el vector que contiene los percentiles que nos interesan, en este caso  deseamos calcular los cinco cuantiles desde Q0 a Q4.

q=c(0,.25,.5,.75,1)

## Paso 2, agrupamos los datos por continente, (recuerde es una variable categ[orica  "factor". Finalmente usamos el verbo summarize para hacer la estimación de los percentiles. Ete calculo se base en la función quantile que toma los argumentos lifeExp y  probs en donde definimos el la columna del vector donde almacenamos los percentiles a estimar )

q=c(.30,.60,.9)
###1952
estimado_y52<-cuantiles%>%
  filter(year=="1952")%>%
  group_by(continent) %>%
  summarize(
percentil30 =  quantile(gdpPercap, probs =q[1]),
percentil60 = quantile(gdpPercap, probs = q[2]),    percentil90 = quantile(gdpPercap, probs = q[3]) )
### 2007
estimado_y07<-cuantiles%>%
  filter(year=="2007")%>%
  group_by(continent) %>%
  summarize(
percentil30 =  quantile(gdpPercap, probs =q[1]),
percentil60 = quantile(gdpPercap, probs = q[2]),    percentil90 = quantile(gdpPercap, probs = q[3]) )

```

##### Ejercicio asignación para casa.

Estime los percentiles 30, 60, 90 para la variable ingreso en el año 1952 y 2007 para los países de los continentes  Africa y  Asia. 

Indique brevemente sus observaciones.


```{r}

y_52Africa<-cuantiles%>%filter(continent=="Africa",year=="1952")
  
names(y_52Africa)
quantile(y_52Africa$gdpPercap)
  
```


#### Rango

Una de las medidas más sencillas para medir la dispersión en la distribución de los datos es mediante la diferencia entre el dato maximo y el mínimo, este indicador se denomina **Rango **

En la estimación del rango se debe tener cuidado con la presencia de outliers (datos extremos o atípicos).


Para tener una idea de la dispersión de los datos, es común estimar el **rango intercuartil** que marca la distancia entre el dato en el percentil 25 y el 75.

En la práctica, el percentil 25 se denomina  **Q1**  o primer cuartil y el percentil 75 se denomina cuartil 3 o **Q3**. 

La estimación del rango intercuartil es comunmente usada como prueba para detectar valores extermos  (**outliers**). 

El criterio de decisión indica que se tiene un outlier si un dato se ubica sobre 1.5 IQR respecto a los limites definidos por  el **Q1** y el **Q3**. 

![](/img/d.jpg)

#### Representación gráfica de la frecuencia.


Un punto de partida básico es mediante la exploración de la frecuencia. Veamos como podemos generar un tabla de frecuencia para conocer la distribución de un conjunto de datos hipotético:
 
 
 
```{r}
# Defining vector
x <- c(3, 7, 5, 13, 20, 23, 39, 23, 40,
	23, 14, 12, 56, 23, 29, 56, 37,
	45, 1, 25, 8, 56, 56)

# Generate frequency table
y <- table(x)

# Print frequency table
print(y)

```


Podemos representar esta información mediante diversas visualizaciónes: 

Tres de las más importantes son:

- **Histogramas**

- **Funciones de Densidad (KDF)**

- **Box plot**


### Histograma

Esta es una representación   visual que integra la frecuencia con la ocurre una determinada observación. (Nota: Es una representación para datos discretos, no contínuos) 

Se compone por la suma de la frecuencia observada entre grupos predefinidos.

La estimación considera dos parámetros: El número de grupos o clases  (bins, barras que integran el histograma) y el ancho de clase. 

Para estimar el primer parámetro, consideramos la raíz cuadrada del número de obsrevaciones en el conjunto de datos.

Para estimar el ancho de clase, dividimos el rango entre el número de clases (bins).

**Contexto**. Creación de histograma para representar la frecuencia de los datos. 

Ir a **.ipynb** file

##### Ejemplo 1

```{r, warning = FALSE}

library(tidyverse)

### Load data. contiene registros de nacimientos con sindrome de dificultad respiratoria. MX, Variables de interés, peso y tipo de parto.  ej cesarea o eutócico.


## from URL

url<-("https://raw.githubusercontent.com/JoseLuisManzanaresRivera/intersemestral-2021/main/content/post/sindrome.csv")

peso<-read.csv(url)

## Hacemos la revision de rutina. 
names(peso)
view(peso)
str(peso)
glimpse(peso)
summary(peso)

```

```{r, include=FALSE}

#N16<-readRDS("N16.rds")
#glimpse(N16)

#p220<-select(N16,cve_cie, ent_nac, con_indm,aten_pren,niv_escol,sexoh,tallah,pesoh,apgarh,silverman,procnac,edad_madre )%>%mutate(sdr=ifelse(cve_cie=="P220"|cve_cie=="P229",1,0), sexoh=as.factor(sexoh))%>%filter(pesoh!=9999, tallah!=99,sdr==1, sexoh!=9,edad_madre!=999, procnac!=9&procnac!=3&procnac!=8&procnac!=4, apgarh<=10, silverman<=10)%>%mutate(proc_n=ifelse(procnac==1,"Eutócico","Cesárea"))%>%rename(Procedimiento=proc_n)

#write.csv(p220,"sindrome.csv")
# getwd()

## p220_no_sdr<-select(N16,cve_cie, ent_nac, con_indm,aten_pren,niv_escol,sexoh,tallah,pesoh,apgarh,silverman,procnac,edad_madre )%>%mutate(sdr=ifelse(cve_cie=="P220"|cve_cie=="P229",1,0), sexoh=as.factor(sexoh))%>%filter(pesoh!=9999, tallah!=99,sdr!=1, sexoh!=9,edad_madre!=999, procnac!=9&procnac!=3&procnac!=8&procnac!=4, apgarh<=10, silverman<=10)%>%mutate(proc_n=ifelse(procnac==1,"Eutócico","Cesárea"))%>%rename(Procedimiento=proc_n)
```

```{r}

### Paso 2 Hacemos el Histogramo. Note que la variable en el eje x es el peso. Por default la variable en Y es la frecuencia en este caso el número de casos.

# Nota2: observe que usamos la función facet_grid  para indicar que necesitamos los histogramas agrupados para la variable categorica  procedimiento.

h1<-ggplot(peso, aes(x=pesoh)) + geom_histogram(fill="white", colour="red") +
facet_grid(Procedimiento~ .)+
xlab("Peso (gr.)")+ylab("Casos")

h1 ## Nota  el argumento colour="namecolor" nos permite asignar un color para las barras del histograma.

### Representación alternativa con los histogramas en un solo plot, en este caso el parametro fill indica la variable categorixa que usaremos para representar los histogramas, en este caso se mostrarán en la misma grafica. Usamos el argumento identity para indiccar que necesitamos solo el histograma para la variable peso.

h2<-ggplot(peso, aes(x=pesoh, fill=Procedimiento)) +
geom_histogram(position="identity")+
xlab("Peso (gramos)")+ylab("Casos")+
scale_fill_manual(values=c("grey70", "grey20"))


## Nota el argumento fill=" " , nos permite incluir una variable categórica (un factor) para obtener sub plots en este caso dos histogramas correspondientes a la categoría procedimiento (cesarea, eutócico). Intuitivamente fill "rellenará" cada categoría con un plot, este argumento se debe acompañar de la opción **scale_fill_manual** que permite definir los colores asignados para cada histograma de la categoria procedimiento.



```


```{r,echo= FALSE, fig.height = 8, fig.width = 12, warning=FALSE, message=FALSE}
h2
```

Importante: La forma del histograma  (en cuanto a su simetría) permite informar sobre **el sesgo de los datos**. 

En la figura siguiente se observan dos esenarios relevantes: **Sesgo positivo**  y **negativo**. En ambos casos se tiene una implicación sobre el supuesto de normaliad de los datos. Recordemos que la simetría es una condición necesaria de una   **distribución normal.** 


Una particularidad adicional es que en ausencia de **normalidad**   de los datos, las medidas de tendencia central **media** y **mediana** difieren.


**Importante**: La distribución normal es útil como veremos al trabajar con estadística inferencial por que  la confiabilidad de algunos estimados  depende de este comportamiento de normalidad. 


![](/img/qq1.jpg)
**Note** que un **patrón concavo** de los datos respecto a la linea de 45° que representa una distribución teórica simétrica (normal), es indicativo de **sesgo positivo**: ej. la cola de la dsitribución de los datos se extiende  **hacia el cuadrante positivo** en dirección cartesina.

Se denomina sesgo **negativo** cuando el patrón observado es **convexo**, respecto a la linea de 45° de la **gráfica QQ** (qqplot).



**Para enfatizar:**  cuando **no tenemos simetría**, las funciones de distribución no son **normales** y observamos sesgos que pueden ser tanto en sentido **positivo** como **negativo**. 

En ambos casos la posición relativa entre media y mediana corresponde con el tipo de sesgo en cuestión. 

Por ejemplo: 

La **media** de un conjunto de datos con **sesgo positivo** se ubica **a la derecha** de la mediana. En el escenario de una distribución con **sesgo negativo**, la media se ubica a la **izquierda** de la **mediana**.

![}(/img/escenarios.jpg)


##### Ejercicio 1 

Determine si la varaiable talla (medida en centimetros) de la base de datos correspondiente a características de recién nacidos, exhíbe una distribución simátrica (normal) de los datos.


[documentación para consulta de referencia sobre la función qqplot](https://www.rdocumentation.org/packages/EnvStats/versions/2.3.1/topics/qqPlot)
```{r}

## Estimación de qqplot para variable tallah, esta mide en centimetros la estatura del bebé.


library(ggpubr)

ggqqplot(peso$tallah)

```

**Ans:** variable con sesgo negativo


**Verificación mediante inspección visual del histograma correspondiente**

```{r}

negativo<-ggplot(peso, aes(x=tallah)) + geom_histogram(fill="white", colour="blue") +
facet_grid(Procedimiento~ .)+xlab("Peso (gramos)")+ylab("Casos")
negativo
```

##### Nota: 

**El teorema de límite central** nos indica que independientente de la distribución de un conjunto de datos, **la distribución muestral** tiende a comportarse acorde a la **distribución normal**, si la muestra es lo suficientemente **grande (n>30).** 


[Material opcional de consulta para practicar la visualización del histograma ](http://www.sthda.com/english/wiki/ggplot2-histogram-plot-quick-start-guide-r-software-and-data-visualization)



## Boxplot.

**Contextualización:** Contraste entre la distribución de grupos,  identificación de **outliers** y permite ubicar  el **rango inter-cuartil** y la **mediana**.

Esta representación visual de la distribución es generalmente utilizada para la representación de **variables categóricas**.

Note que los upper and **lower fences** o **whiskers**, se extienden por definición en un rango de +/- 1.5 IQ, pero la representación considera los puntos más alejados **dentro de este rango**, es decir, puede ser que el rango sea mayor a la ubicación concreta de los puntos en cada lado, es por ello que los **fences** comunmente no son simétricos respecto a los límites de la caja que forma el rango intercuartil.

### Representación de dispersión datos con frecuencia diaria. Boxplot



![](/img/explicacion_bxplt.jpg)


**Ejemplo 1**  

Ver Sesion2.ipynb


**Ejemplo 2**

Estimación de **box plot** para examinar la distribución de una variable. 


Inclusión de sólo una categoría. 


Distribución de temperaturas mínimas diarias registradas en estación metereológica **DGO10092** del SMN. Periodo 1941 -2000.

[Base de datos para el ejercicio](https://drive.google.com/file/d/1zIZyX4ZAFPwjgqcTT6rSUgzNutJok_wD/view?usp=sharing)

[Créditos de la fuente de información](https://smn.conagua.gob.mx/es/climatologia/informacion-climatologica/normales-climatologicas-por-estado)

```{r,  fig.height = 8, fig.width = 12,  echo = FALSE, warning = FALSE}

# Paso 1 get data from url

#urldgo<-"https://raw.githubusercontent.com/JoseLuisManzanaresRivera/intersemestral-2021/main/content/post/dgo_temp.csv"

#dgo_temp<-read.csv(urldgo)
## Load data set form From local

#dgo_temp<-readRDS("dgo_temps.rds")




## Damos un vistazo a los datos para comprobar que las variables  estan en el formato apropiado para su análisis.
```


```{r}
# Paso 1 get data
dgo<-read.csv("dgo_datos_clima.csv")

getwd()
### Paso 2. Exploramos con el procedimiento estandar.
names(dgo)
view(dgo)
glimpse(dgo)
str(dgo)
```

```{r, include=FALSE}

## Notamos que la variable FECHA no esta declarada como clase "date", necesitamos hacer un mutate para corrgir esto antes de su uso.  
#Tip: Usamos la función as.Date respetando el orden de los datos como día, mes, año !!

#DGO<-dgo_temp%>%mutate(date=as.Date(FECHA, "%d/%m/%Y"))%>%mutate(ms=as.integer(substr(FECHA,4,5)),año=substr(FECHA,7,10))

## Creamos un data frame con dos variables: mes con el nombre del mes  y ms con el número del mes para agregarlo a la base original. Esto será necesario para su representación gráfica. 


#meses<-data.frame(mes=c("Enero ",	"Febrero",	"Marzo",	"Abril",	"Mayo",	"Junio",	"Julio",	"Agosto",	"Septiembre",	"Octubre",	"Noviembre",	"Diciembre"),ms=seq(1, 12, by=1))


### Usamos la función left_join para unir las dos bases de datos (data frames). 

#DGO<-left_join(meses,DGO)
##glimpse(DGO)
##levels(DGO$mes )
#write.csv(DGO, "dgo_temp.csv")

## Plot Daily  1941-2000
```

```{r}
## Paso 3. Creamos el Plot para cada mes con datos diarios en el periodo. 1941-2000

boxplot<-ggplot(dgo,aes(x=fct_inorder(meses),y=tempmin))+
geom_boxplot()+
xlab("Mes")+
ylab("Temperatura diaria mínima°c")+
theme(axis.text.x = element_text(angle=45, hjust=1, vjust=1))
```
#### Distribución de temperaturas mínimas  diarias registradas en estación metereológica DGO10092   del SMN. Periodo 1941 -2000. 

```{r, warning = FALSE}
boxplot
```

**Ejemplo 2:** 

Determinación de Media, Rango intercuartil y percentiles 25 y 75  Q1 y Q3 (p25 y p75). 
Representación con boxplot para variables tipo categóricas. 


```{r,  fig.height = 8, fig.width = 12, include=FALSE,  echo = FALSE, warning = FALSE}

#BC2001<-read.csv("ema_2001_AGUACALIENTE.csv")%>%rename(max=tmin,min=tmax)

#df<-BC2001%>%mutate(max=as.numeric(as.character(max)),min=as.numeric(as.character(min)) )

#temp<-mutate(df,date=as.Date(FECHA, "%d/%m/%Y"))%>%mutate(mes=as.integer(substr(FECHA,4,5)),año=substr(FECHA,7,10))

#tidy<-gather(temp, tipo, temperatura, max:min)


#meses<-data.frame(Meses=c("Enero ",	"Febrero",	"Marzo",	"Abril",	"Mayo",	"Junio",	"Julio",	"Agosto",	"Septiembre",	"Octubre",	"Noviembre",	"Diciembre"),mes=seq(1, 12, by=1))


#tidy_bc_temp<-left_join(tidy,meses)

#write.csv(tidy_bc_temp, "tidy_bc_temp.csv")
```


```{r, include=FALSE}
## Paso 1 Load data from local
### Alternativa para uso de url



## Paso 2  Make boxplot.
```


```{r}
library(tidyverse)
library(forcats)
library(plotly)

# 1.- Descargar los datos desde url

eprueba<-read.csv("https://raw.githubusercontent.com/JoseLuisManzanaresRivera/intersemestral-2021/main/content/post/bajacalifornia.csv")


boxplot_2<-ggplot(eprueba, aes(x=fct_inorder(nombremes), y=t)) +
geom_boxplot(aes(fill=tipo))+
xlab("Mes")+
ylab("Temperatura diaria máxima y mínima°c")+
theme_light()+
theme(axis.text.x = element_text(angle=45, hjust=1, vjust=1))

ggplotly(boxplot_2)
```


```{r}

# Alternativa si descargan  la base  a su lap y lo guardan en la misma carpeta donde tienen el  la sesión del   Rmarkdown  (.Rmd)

library(tidyverse)
library(forcats)
library(plotly)

# 1.- leer el archivo

eprueba<-read.csv("bajacalifornia.csv")

# 2 .- Crear la gráfica de bigotes (Boxplot)

boxplot_2<-ggplot(eprueba, aes(x=fct_inorder(nombremes), y=t)) +
geom_boxplot(aes(fill=tipo))+
xlab("Mes")+
ylab("Temperatura diaria máxima y mínima°c")+
theme_light()+
theme(axis.text.x = element_text(angle=45, hjust=1, vjust=1))

ggplotly(boxplot_2)
```



*Documentación adicional con ejemplos de repaso* [boxplot](http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/)


En síntesis es posible describir una distribución a partir de 5 medidas resumen estas nos permiten contar con elementos para comparar entre la distribución de conjuntos de datos:

Q0 (min), Q1, media, Q3, Q100 (max).

Las herramientas visuales del **Histograma** y los **diagramas de caja**, complementan la información al permitir una valoración directa de la forma de la distribución así como la detección de **valores atípicos**.


### Varianza

El valor promedio de las desviaciones al cuadrado de cada elemento *i*  respecto a la media.

En términos muestrales tenemos: 


$$s^2_x = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{X})^2$$

### Grados de libertad


**El concepto**

Nota que para calcular la varianza $s^{2}$ dividimos la sumatoria entre el **número de observaciones menos uno**, es decir, por **n − 1**, en vez de **n**. 

La razón es que la suma de las desviaciones $x_{i} − \bar{X}$ es siempre cero, la última desviación se puede hallar cuando se conocen las otras **n − 1**. 

Solamente **n − 1** de las desviaciones al
cuadrado pueden variar **libremente**. 
______________________________________________________________

Esta es la razón por la que dividimos entre
**n − 1**. 

Al número **n − 1** se le denomina **grados de libertad** de la varianza o grados de 
libertad de la desviación estándar. 



$$V(X)= Var(X)=\sigma^2_{X}$$

La varianza es una referencia importante para determinar la dispersión de un conjunto de datos. 

Sin embargo, cuando queremos comparar las dispersión entre conjuntos de datos una deventaja de la varianza es que esta exprezada en **unidades al cuadrado**.  

Es una medida de dispersión que depende de la escala de medición.


Esta desventaja la resuelve la **desviación estándar** que es una medida independiente de la escala.  

Por este motivo la medida de dispersión **clásica** que utilizaremos para comparar la dispersión de los datos es la **desviación estádar**. 

### Desviación estándar. 

Raíz cuadrada de la varianza.

$$\sqrt{\sigma^2} = \sigma$$

O bien solo **S** Cuando nos referimos al estadistico (**muestral**)

Esta medida, la podemos estimar usando la función `sd()` en **R** com oargumento incluimos la bse de datos y el nombre de la columna que contiene la variable númerica de la cual vamos a obtener la desviación estándar.   

**Por ejemplo**, si mi base de datos se nombra  **x**  y dentro de este  **data frame** tengo una variable denominada **ingreso** de la cual, que me interesa conocer la desviación estándar, entonces la puedo estimar con la siguiente especificación `sd(x$ingreso)`. 

----------------------------------------------


Asociado a esta medida, podemos calcular **el coeficiente de variación** que solo añade la relación  de esta dispersión, medida por la **desviación estándard** respecto a la **media.**

Este coeficiente se ubica en el rango 0-1 frecuentemente, pero en algunas distribuciones puede superar la unidad. 
$$cv=\frac{\sigma_x}{|\bar{X}|}$$
En términos generales entre mayor es el **cv**, tenemos una mayor dispersión en los datos. 

+ Un rasgo importante de la desviación estándar es que es particularmente sensible a la presencia de datos atípicos   (valores extremos o *outliers*). 

Tenemos que ser cautelos al interpretar **S** como una medida de dispersión confiable si obervamos la presencia de datos atípicos en nuestro conjunto de datos. 


## Actividad 


Con los datos  disponibles sobre precipitaciones registrados en la estación meteorológica  en [Tijuana](https://drive.google.com/file/d/1ZiBDa7FGMc9_fdGCIV76AzNVQ0LMey7m/view?usp=sharing), determine: 

+ ¿Qué valor toma la media, la mediana, Q1, Q3 y cuál es el rango intercuartil?

+ ¿Qué año presenta la mayor precipitación?

+ Represente graficamente la trayectoria de la variable.  

+ ¿Qué valor toma la desviación estándar?




```{r}

# Descarga directa desde url

practica<-read.csv("https://raw.githubusercontent.com/JoseLuisManzanaresRivera/intersemestral-2021/main/content/post/tijuana.csv")

names(practica)
```


```{r}

### Paso 1  agrupamos por fecha y sumamos los datos de precipitación.

precip<-group_by(practica,fecha)%>%
  summarise(lluvia=sum(precipitacion))

### Paso 2  Estimar estdísticas descriptivas básicas.

summary(precip)

## Para estimar rango intercuartil

356.9 -163.9


## Para estimar la desviación estándar.

sd(precip$lluvia)

## Convertimos variable fecha de Chr string a numérica para poder hacer  representación gráfica. 

precip<-mutate(precip,fecha=as.numeric(fecha))

### paso 3 plot Representación de la trayectoria promedio.
```


```{r}

plt<-ggplot(precip, aes(x=fecha, y=lluvia))+
geom_line(size=.3, colour="gray")+
geom_smooth()+
xlab("Año")+
ylab("Precipitación  promedio. 1969-2012 (mm)")+
theme_light()+
geom_hline(yintercept = 185.2, size=.5, linetype="dashed",color='red')+
geom_hline(yintercept = 343.5, size=.5, linetype="dashed",color='darkblue')

ggplotly(plt)

```




```{r, include=FALSE}

#Note que se tiene una linea de tendencia, esta resulta de la aplicación del parámetro **geom_smooth** este actua como un filtro que permite observar la tendencia de la temperatura a lo largo de la serie de tiempo. 

#El estimado que se logra con la función geom_smooth *"suaviza"* el comportamiento de la variable en niveles, (línea en gris). 

#En el *background*  el parámetro **geom_smooth** realiza una **regresión lineal por el método de mínimos cuadrados ordinarios** (un tema que desarrollaremos a detalle en la sección siguiente), usando la especificación **LOESS** (locally estimated scatterplot smoothing). 


#Su aplicación resulta en un método de visualización que permite determinar el comportamiento o tendencia de la variable de interés. 
```



## Actividad.

Revise la página 42  sección: "aplica tus conocimientos", del libro de texto  [Estadística aplicada básica](https://drive.google.com/file/d/1sbwcTP-k7_EKmY406BVYUwLiMZENKzCs/view?usp=sharing) y realiza los ejercicios.  **Ej 1.31, 1.32, 1.33**

-------------------------------------


 
## Precisiones sobre la distribución normal.

Una **caso particular de la distribución esta dado cuando observamos simetría** (ej. media, mediana son igueales). 

En este caso, ambos lados de la función de densidad integran **la misma proporción de datos**. 

En términos de su forma, estas curvas además de ser **simétricas** tiene un sólo pico, (no tiene dos **modas**  no son bimodales) y tienen forma de **campana**.

La curva de densidad de una distribución de datos **normal**, se describe completamente a partir de su media $\mu$ y desviación estándar $\sigma$ 

Y la podemos denotar $N(\mu, \sigma)$

Considere los dos ejemplos siguientes, **ambos son escenarios de distribuciones normales**, aunque el grado de dispersión es mayor en el primer caso.

El parámetro $\sigma$ controla el grado de dsipersión de la **curva normal**. 

![](/img/normalidad.jpg)

En términos gráficos, el **punto de inflexión**, se ubica a una distancia de magnitud  $\sigma$ respecto de $\mu$.

![](/img/inflexion.jpg)

## Paricularidades de la distribución normal.

Una particularidad relevante de las distribuciones normales es que cumplen con la siguiente regla:

+ El **68%** de los datos se localiza a una $+,- \sigma$  desviación estándar,  de la media $\mu$. 

+ El **95%** de los datos se localiza a $+,- 2\sigma$  de la media $\mu$

+ El **99.7%** de los datos se localiza a $+,- 3\sigma$ de la media $\mu$

![](/img/regla_normal.jpg)

### ¿Por qué tanto énfasis en las distribciones normales? 

+ Porque describen una gran cantidad de situaciones empíricas de naturaleza aleatoria.

+ Los procesos de **inferencia estadística**   permiten resultados confiables (robusos) cuando se aplican a distribuciones simétricas.
Dependen de esta condición.  

##### Actividad para afianzar los conceptos

Conteste las siguientes preguntas

![](/img/preguntas.jpg)


Para finalizar esta sección es importante notar que es frecuente usar los **valores estandarizados** de la distribución normal con fines de comparación de las distribuciones. 

Estandarizar en este caso consiste en tomar un valor particular del conjunto de datos, restarle el valor de la media y dividirlo entre la desviación estándar. 

A este valor estandarizado se le denomina **Z Score** y se obtiene  $$z=\frac{x-\mu}{\sigma}$$ 

Este **score z** nos indica a **cuántas desviaciones estándar** se encuentra la observación original de la **media** y en qué dirección. 

Por **ejemplo**, considerando los datos del ejercicio anterior, vemos que la media $\mu=110$, $\sigma=25$, por lo que el coeficiente estandarizado de una persona cuyo IQ= 150, será $$z=\frac{150-110}{25}=1.6$$.  

Y se interpreta como la distancia en términos de desviaciones estándar respecto a la media. Así el dato IQ de 150 se ubica a una distancia de 1.6 desviaciones estandar de la media. 


Las observaciones **mayores** que la media
son positivas y las **menores**, negativas.
 
 
Resumiendo: El **proceso de estandarización** unicamente permite exprezar los valores de las distribución en relación a una **escala común**, en este caso, una **distancia respecto a la media**.

En términos generales la **distribución normal estandarizada**, se expreza  con los parámetros media cero y desviación estándar =  1 o bien $$N(0,1)$$ $$z=\frac{x-\mu}{\sigma}$$


### Ejercicios.

1.-¿


 **Tarea. Se recomineda revisar el material del libro de texto:** 

[Libro de texto](https://drive.google.com/file/d/1sbwcTP-k7_EKmY406BVYUwLiMZENKzCs/view?usp=sharing) sección 1.44, páginas 64-79.





### Distribuciones contínuas.

Muy a menudo, encontramos datos que no se limitan a cifras enteras, por ejemplo el tipo de cambio **Peso/USD**, los registros de temperatura, en estos casos necesitamos una función contínua para describir su distribución.


### KDF Función de densidad Kernel

Permite la representación de la districuión considerando una aproximación contínua,no discreta como es el caso del histograma. 

**Notación:** 
![](/img/kernelDF.jpg)
Donde **K** es una función, usualmente la distribución normal (distribución con  $$\mu=0$$ y $$\sigma=1$$).

En notación esto es: 
$$K(x)=\phi (x)$$ con $\phi$ como la distribución normal.


**h** representa el ancho de clase y toma un valor positivo. $$[0,\infty]$$ Es un parámetro que puede modificarse para generar una distribución con mayor o menor suavidad en la curva. 

Los extremos o limites para **h** son 0, con una curva de densidad sin suavidad e infinito totalmente plana. 

Ejemplo. 

![](/img/kernelnormal.jpg)

**Ejemplo 1** Estimación de funcion de densidad para representar la frecuencia de datos para variables numéricas  y su visualización  por categorias. 
Herramienta de visualización: **ggplotly**.  Funciones aplicadas: **ggplot** **geom_density**.

Contexto de la base de datos. Data frame en formato rds que contiene Edad de  pacientes positivos por covid y edad de defunción en México. Año 2020.

##### Ejemplo 1. 

**Cálculo función de densidad, representación de medidas de tendencia central  y distribución para detectar diferencias de edad.**

```{r, echo= FALSE, fig.height = 5, fig.width = 8.5}

## Source: https://www.gob.mx/salud/documentos/coronavirus-covid-19-comunicado-tecnico-diario-238449

agedf<-readRDS("agedf.rds")


dp<-ggplot(agedf, aes(x=edad, fill=sex))+
geom_density(alpha=.2)+  
labs(color="Sex")+
xlab("Patient age  (years)")+
ylab("Density  f(y)")+
theme_classic()+
scale_fill_discrete(labels=c("Female", "Male"))+
geom_vline(xintercept =mean(agedf$edad), linetype="dotted", color="red")+
annotate("text", x=32, y=.005, size=2,label="Mean patient age")+ theme(legend.title=element_blank())

ggplotly(dp)
```
**Ejemplo 2**  Distribución de edades para defunción por covid en México. 
Funciones estudiadas (Verbos tidyverse aplicados): Filter, select, mutate, summarise. ggplot 

```{r, echo= FALSE}

open<-readRDS("open.rds")

def<-filter(open, resultado==1)%>%
  filter(fecha_def!="9999-99-99")%>%
  select(sexo, edad)
def$sexo[def$sexo == "1"] <- "Female"
def$sexo[def$sexo == "2"] <- "Male"  
def<-mutate(def,sexo=as.factor(sexo))

fmean<-filter(def, sexo=="Female")%>%
  mutate(varmean=mean(edad))%>%
   summarise(mean=mean(varmean))

mmean<-filter(def, sexo=="Male")%>%
  mutate(varmean=mean(edad))%>%
   summarise(varmean=mean(varmean))

  
p<-ggplot(def, aes(x=edad, fill=sexo))+
geom_density(alpha=.2)+  
labs(color="Sex")+
xlab("Person age  (years)")+
ylab("Density  f(y)")+
theme_classic()+
scale_fill_manual(values = c("brown","blue4"))+
geom_vline(xintercept = fmean$mean, linetype="dotted", color="purple")+
geom_vline(xintercept = mmean$varmean, linetype="dotted", color="blue")+
annotate("text", x=44, y=.005, size=3,label="Mean person age")+ theme(legend.title=element_blank())


ggplotly(p)
```


**Ejemplo  3** Función de densidad para dos categorias. 
Funciones estudiadas: group_by, rename, left_join, filter, select, mutate, summarise. ggplot 

####  Caso Aguascalientes


```{r,  fig.height = 8, fig.width = 12,  message=FALSE, warning=FALSE, echo=FALSE}


sdensity<-readRDS("density.rds")%>%
  mutate(s=as.factor(s))%>%
  filter(ENT_RESID!=33& ENT_RESID!=34& ENT_RESID!=35& ENT_RESID!=99)%>%
  group_by(ENT_RESID)

cat_e<-read.csv("cat_entidad.csv")%>%
select(-X)%>%
rename(ent_res=X.U.FEFF.EDO)%>%
mutate(ent_res=sprintf("%02d",ent_res))%>%
mutate(ENT_RESID=as.factor(ent_res))%>%
select(-ent_res)

sdensity<-left_join(sdensity,cat_e)



ags<-filter(sdensity,ENT_RESID=="01")%>%
  mutate(s=as.factor(s))


aplt<-ggplot(ags, aes(x=age,linetype=s, color=s))+
geom_line(stat="density")+
labs(linetype="Causa")+
xlab("Edad de la persona (Años)")+
ylab("Densidad f(y)")+
annotate("segment", x=29, xend=13, y=0.011, yend=0.011,  size=.3, arrow=arrow(length=unit(.2,"cm")))+
annotate("text", x=23, y=0.013, label="Alta incidencia")+
annotate("text", x=21, y=0.009, label="13<Edad<29")+
annotate("rect", xmin=13, xmax=29, ymin=0, ymax=0.03, alpha=.1,fill="black")+
scale_linetype(labels=c("Otras causas","Suicidio"))+
theme_classic()+
scale_color_manual(values = c("mediumturquoise","magenta1"))+
theme(legend.position="none")


aplt

```


**Ejemplo 4** Función de densidad multiples bases de datos. (32 estados de México).
Herramienta de análisis: stat=density 

Funciones estudiadas: **facet_wrap**

```{r,  fig.height = 8, fig.width = 12, message=FALSE, warning=FALSE, echo =FALSE}


mxplt<-ggplot(sdensity, aes(x=age,linetype=s, color=s))+
geom_line(stat="density")+
labs(linetype="Causa")+
xlab("Edad de la persona (Años)")+
ylab("Densidad f(y)")+
facet_wrap(~DESCRIP, nrow=6)+
scale_linetype(labels=c("Otras causas","Suicidio"))+
  theme_classic()+
scale_color_manual(values = c("mediumturquoise","magenta1"))+
  theme(legend.position="none")

mxplt
```



**Nota sobre ventajas de las KDF: **

Nos permiten distinguir rapidamente características como los atributos de tendencia central, la presencia de  bimodalidad,  sesgo), también facilitan la comparación rápida entre diferentes conjuntos de datos.

Por ejemplo vemos en la figura siguiente que la **media** de un conjunto de datos con **sesgo positivo** se ubica **a la derecha** de la mediana.

**IMPORTANTE ** En este caso **la mediana** será la medida de tendencia central **preferible** o que describe con mayor precisión la ubicación del **dato central**, por que divide la distribución de la curva en dos areas iguales. 

![](/img/asimetria_positiva.jpg)

**Una propiedad importante:**

*"El área por debajo de la curva, y entre cualquier intervalo de valores, es la proporción de todas las observaciones que están situadas en dicho intervalo."*

+ Note que el area por debajo de la KDF, siempre es igual a 1. 

En otras palabras, si sumamos la proporción que integra la distribución total, tenemos el 100% 


###  Función de distribución de probabilidad PDF.

La curva que describe la probabilidad para cada valor, (la distribución de probabilidad),  es una **función contínua** y se denomina función de **densidad de probabilidad.** $p(x)=$**(PDF).** 


La **PDF** (función de densidad de una variable **aleatoria continua**), es una función que describe la probabilidad de que una variable aleatoria **X** tome un valor particular **x**  

**X** Variable aleatoria ej. ingreso
**x:** valor particular de la variable aleatoria. ej. $60 000.

La pregunta es ¿cuál es la probabilidad de que la variable **X** tome un valor particular **x**

La **PDF** tiene las propiedades: 


> $${p(x)}\geq0, 	\forall x \in R$$
> $$\int_{-\infty}^{\infty}     p{(x)}dx=1$$

![](/img/PDF_probability.jpg)

La integral de p(x) en el rango [a,b] representa la probabilidad de encontrar el valor **x** en ese **intervalo**.  El area bajo la curva, determina el valor de esa probabilidad en el intervalo particular. A esta area la denominamos función de densidad acumulada **CDF** 


## Ejercicio.

![](/img/ejerciciodensidad.jpg)


##### Aplicación de la **CDF** a la estimación de los **percentiles.**

La manera más simple de determinar los percentiles es mediante la estimación de la **función de densidad acumulada** **CDF** (por sus siglas en inglés). 
$$CDF(x)=\int_{-\infty}^x PDF(x)dx$$

#### ECDF Función de densidad acumnulativa empírica.

Conocer la frecuencia es importante pero muy a menudo necesitamos conocer que proporción de nuestras observaciones se ubica por debajo de un cierto umbral.

Para este propósito es muy útil estimar la frecuencia acumulada. 

Una representación contínua de este concepto es mediante la función empírica de **densidad  acumulada** 


Una nota sobre la apliación de la **CDF** en la obtención de los **Percentiles** 

Los percentiles permiten determinar el valor por debajo del cual, ocurre un determinado porcentaje de los datos.

La  función de distribución acumulada**CDF** es la integral de la **función de densidad de probabilidad (PDF)**. 

Define la probabilidad de observar un valor **x** en un rango determinado **[a,b].** 
Formalmente 

![](/img/cdf.jpg)


Revisemos ahora en la práctica como podemos estimar la **función de densidad**  usando **Python.**

**"Sesión_2_Tendencia central.ipynb"**

Nota sobre implementación en **Python**. Esta  nos permite observar el porcentaje acumulado de datos en el eje de las ordenadas al orígen y los valores observados en el eje de las absisas.

La pregunta básica que podemos responder estimando la **CDF**, es ¿qué porcentaje de los datos se ubica por debajo de cualquier umbral deseado?
