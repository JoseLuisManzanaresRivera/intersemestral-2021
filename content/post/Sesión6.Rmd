---
title: "Sesión 6"
author: "Jose´Luis Manzanares Rivera"
date: '2022-11-01T21:13:15-05:00'
---



```{r, message=FALSE,warning=FALSE,include=FALSE}
library(tidyverse)
library(datagovindia)
library(plotly)
```


```{r, message=FALSE,warning=FALSE,include=FALSE}
library(tidyverse)
library(datagovindia)
library(plotly)
library(gapminder)
```


## Continuación unidad 1. 

## Actividad 


Con los datos  disponibles sobre precipitaciones registrados en la estación meteorológica  en [Tijuana](https://drive.google.com/file/d/1ZiBDa7FGMc9_fdGCIV76AzNVQ0LMey7m/view?usp=sharing), determine: 

+ ¿Qué valor toma la media, la mediana, Q1, Q3 y cuál es el rango intercuartil?

+ ¿Qué año presenta la mayor precipitación?

+ Represente graficamente la trayectoria de la variable.  

+ ¿Qué valor toma la desviación estándar?




```{r}

# Descarga directa desde url

practica<-read.csv("https://raw.githubusercontent.com/JoseLuisManzanaresRivera/intersemestral-2021/main/content/post/tijuana.csv")

names(practica)
```


```{r}

### Paso 1  agrupamos por fecha y sumamos los datos de precipitación.

precip<-group_by(practica,fecha)%>%
  summarise(lluvia=sum(precipitacion))

### Paso 2  Estimar estdísticas descriptivas básicas.

summary(precip)

## Para estimar rango intercuartil

356.9 -163.9


## Para estimar la desviación estándar.

sd(precip$lluvia)

## Convertimos variable fecha de Chr string a numérica para poder hacer  representación gráfica. 

precip<-mutate(precip,fecha=as.numeric(fecha))

### paso 3 plot Representación de la trayectoria promedio.
```


```{r}

plt<-ggplot(precip, aes(x=fecha, y=lluvia))+
geom_line(size=.3, colour="gray")+
geom_smooth()+
xlab("Año")+
ylab("Precipitación  promedio. 1969-2012 (mm)")+
theme_light()+
geom_hline(yintercept = 185.2, size=.5, linetype="dashed",color='red')+
geom_hline(yintercept = 343.5, size=.5, linetype="dashed",color='darkblue')

ggplotly(plt)

```




```{r, include=FALSE}

#Note que se tiene una linea de tendencia, esta resulta de la aplicación del parámetro **geom_smooth** este actua como un filtro que permite observar la tendencia de la temperatura a lo largo de la serie de tiempo. 

#El estimado que se logra con la función geom_smooth *"suaviza"* el comportamiento de la variable en niveles, (línea en gris). 

#En el *background*  el parámetro **geom_smooth** realiza una **regresión lineal por el método de mínimos cuadrados ordinarios** (un tema que desarrollaremos a detalle en la sección siguiente), usando la especificación **LOESS** (locally estimated scatterplot smoothing). 


#Su aplicación resulta en un método de visualización que permite determinar el comportamiento o tendencia de la variable de interés. 
```



## Actividad.

Revise la página 42  sección: "aplica tus conocimientos", del libro de texto  [Estadística aplicada básica](https://drive.google.com/file/d/1sbwcTP-k7_EKmY406BVYUwLiMZENKzCs/view?usp=sharing) y realiza los ejercicios.  **Ej 1.31, 1.32, 1.33**

-------------------------------------


 
## Precisiones sobre la distribución normal.

Una **caso particular de la distribución esta dado cuando observamos simetría** (ej. media, mediana son igueales). 

En este caso, ambos lados de la función de densidad integran **la misma proporción de datos**. 

En términos de su forma, estas curvas además de ser **simétricas** tiene un sólo pico, (no tiene dos **modas**  no son bimodales) y tienen forma de **campana**.

La curva de densidad de una distribución de datos **normal**, se describe completamente a partir de su media $\mu$ y desviación estándar $\sigma$ 

Y la podemos denotar $N(\mu, \sigma)$

Considere los dos ejemplos siguientes, **ambos son escenarios de distribuciones normales**, aunque el grado de dispersión es mayor en el primer caso.

El parámetro $\sigma$ controla el grado de dsipersión de la **curva normal**. 

![](/img/normalidad.jpg)

En términos gráficos, el **punto de inflexión**, se ubica a una distancia de magnitud  $\sigma$ respecto de $\mu$.

![](/img/inflexion.jpg)

## Paricularidades de la distribución normal.

Una particularidad relevante de las distribuciones normales es que cumplen con la siguiente regla:

+ El **68%** de los datos se localiza a una $+,- \sigma$  desviación estándar,  de la media $\mu$. 

+ El **95%** de los datos se localiza a $+,- 2\sigma$  de la media $\mu$

+ El **99.7%** de los datos se localiza a $+,- 3\sigma$ de la media $\mu$

![](/img/regla_normal.jpg)

### ¿Por qué tanto énfasis en las distribciones normales? 

+ Porque describen una gran cantidad de situaciones empíricas de naturaleza aleatoria.

+ Los procesos de **inferencia estadística**   permiten resultados confiables (robusos) cuando se aplican a distribuciones simétricas.
Dependen de esta condición.  

##### Actividad para afianzar los conceptos

Conteste las siguientes preguntas

![](/img/preguntas.jpg)


Para finalizar esta sección es importante notar que es frecuente usar los **valores estandarizados** de la distribución normal con fines de comparación de las distribuciones. 

Estandarizar en este caso consiste en tomar un valor particular del conjunto de datos, restarle el valor de la media y dividirlo entre la desviación estándar. 

A este valor estandarizado se le denomina **Z Score** y se obtiene  $$z=\frac{x-\mu}{\sigma}$$ 

Este **score z** nos indica a **cuántas desviaciones estándar** se encuentra la observación original de la **media** y en qué dirección. 

Por **ejemplo**, considerando los datos del ejercicio anterior, vemos que la media $\mu=110$, $\sigma=25$, por lo que el coeficiente estandarizado de una persona cuyo IQ= 150, será $$z=\frac{150-110}{25}=1.6$$.  

Y se interpreta como la distancia en términos de desviaciones estándar respecto a la media. Así el dato IQ de 150 se ubica a una distancia de 1.6 desviaciones estandar de la media. 


Las observaciones **mayores** que la media
son positivas y las **menores**, negativas.
 
 
Resumiendo: El **proceso de estandarización** unicamente permite exprezar los valores de las distribución en relación a una **escala común**, en este caso, una **distancia respecto a la media**.

En términos generales la **distribución normal estandarizada**, se expreza  con los parámetros media cero y desviación estándar =  1 o bien $$N(0,1)$$ $$z=\frac{x-\mu}{\sigma}$$


 **Tarea. Se recomineda revisar el material del libro de texto:** 

[Libro de texto](https://drive.google.com/file/d/1sbwcTP-k7_EKmY406BVYUwLiMZENKzCs/view?usp=sharing) sección 1.44, páginas 64-79.

**Ejercicio**

![](/img/Z-scores.jpg)

UNa vez que sabemos cómo estandarizar un valor determinado. Es posible usar este **Z-score** para determinar 

##### ¿qué proporción de los datos estan por debajo de este valor estandarizado z-score?

![](/img/tablanormal.jpg)

Las tablas de la distribción Z, dan esta información pero, actualmente es poco frecuente usar estas tablas en lugar de esto podemos estimarlo usando un software. 

Ejemplo: 

¿Qué proporción de los paises en el continente americano tienen un ingreso percapita superiro a 10 000 USD?

Pasos para responder la pregunta.

1.- Estimar Media y desviación estandar

```{r}
zscore<-gapminder%>%filter(continent=="Americas")

summary(zscore)
sd(zscore$gdpPercap)

```

La media $\mu=7136$  y la desviación estándar es $\sigma=6396.764$
Entonces la caracterización de la distribución esta dada por (7136, 6396.764)

**Paso 2**

Estandarizamos el valor solicitado en el problema a z-score.  Sabemos que $$z=\frac{x-\mu}{\sigma}$$, entonces



```{r}

z=(10000-7136)/6396.764
z

```



El  **z-score** para un ingreos de 10 000 es de  **0.4477264**, lo que indica que la distancia, **en desviaciones estándar** desde la **media**, para un ingreso de 10 000 es de 0.4477264. 

**Paso 3** Estimar la proporción por debajo de la curva para el valor estandarizado. 

Con esta información determinamos la proporción de paises cuyo ingreso es inferior buscando los datos en la [tabla A pagina 786 del libro](https://drive.google.com/file/d/1sbwcTP-k7_EKmY406BVYUwLiMZENKzCs/view?usp=sharing).


**Alternativamente podemos usar R para determinar este valor.**

El argumento requerido por la función en R es el **z-score** previamente calculado y el resultado es la proporción de observaciones por debajo de la curva a la izquierda de este valor. 

```{r}

pnorm(0.4477264)

pnorm(10000,7136,6396.764)
```


O en otras palabras la proporción de observaciones con ingreso menor a 10 000. En este caso es del $67\%$

  **Tip** Si conocemos la caracterización de la función podemos usar irectametne la función `pnorm` para estimar también el **z-score**. ej.  pnorm(z-score, $\mu$ , $\sigma$)

## Ejercicios de repaso. 



![](/img/mediaymediana2.jpg)
![](/img/mediaymediana.jpg)

![](/img/ejerciciodensidad.jpg)




### Distribuciones contínuas.

Muy a menudo, encontramos datos que no se limitan a cifras enteras, por ejemplo el tipo de cambio **Peso/USD**, los registros de temperatura, en estos casos necesitamos una función contínua para describir su distribución.


### KDF Función de densidad Kernel

Permite la representación de la districuión considerando una aproximación contínua,no discreta como es el caso del histograma. 


![](/img/den_hist.jpg)



Note que el área bajo la curva de la **función de densidad** representa la proporción de observaciones en un intervalo concreto. Siendo el **area total igual a 1**. 

En análogía a la información que aporta el **histograma** que es una representación  **discreta** como ya hemos estudiado   



**Notación:** 
![](/img/kernelDF.jpg)
Donde **K** es una función, usualmente la distribución normal (distribución con  $$\mu=0$$ y $$\sigma=1$$).

En notación esto es: 
$$K(x)=\phi (x)$$ con $\phi$ como la distribución normal.


**h** representa el ancho de clase y toma un valor positivo. $$[0,\infty]$$ Es un parámetro que puede modificarse para generar una distribución con mayor o menor suavidad en la curva. 

Los extremos o limites para **h** son 0, con una curva de densidad sin suavidad e infinito totalmente plana. 

Ejemplo. 

![](/img/kernelnormal.jpg)

**Ejemplo 1** Estimación de funcion de densidad para representar la frecuencia de datos para variables numéricas  y su visualización  por categorias. 
Herramienta de visualización: **ggplotly**.  Funciones aplicadas: **ggplot** **geom_density**.

Contexto de la base de datos. Data frame en formato rds que contiene Edad de  pacientes positivos por covid y edad de defunción en México. Año 2020.

##### Ejemplo 1. 

**Cálculo función de densidad, representación de medidas de tendencia central  y distribución esperanza de vida países en Africa vs. los paises de Asia.

```{r, echo= FALSE, fig.height = 5, fig.width = 8.5}


library(gapminder)

base<-gapminder

datos<-gapminder%>%filter(continent=="Asia" | continent=="Africa")

groups<-datos%>%group_by(continent)%>%
  summarize(le=mean(lifeExp))

names(datos)
glimpse(datos)

p<-ggplot(datos, aes(x=lifeExp, fill=continent))+
geom_density(alpha=.2)+  
labs(color="Continente")+
xlab("GDPpercap")+
ylab("Density  f(y)")+
theme_classic()+
geom_vline(xintercept= 49, linetype="dotted", color="red")+
geom_vline(xintercept= 60, linetype="dotted", color="blue")+
annotate("text", x=49, y=.005, size=2,label="Esperanza de vida (años)")+ theme(legend.title=element_blank())

ggplotly(p)

```



```{r, include=FALSE}

## Source: https://www.gob.mx/salud/documentos/coronavirus-covid-19-comunicado-tecnico-diario-238449

agedf<-readRDS("agedf.rds")


dp<-ggplot(agedf, aes(x=edad, fill=sex))+
geom_density(alpha=.2)+  
labs(color="Sex")+
xlab("Patient age  (years)")+
ylab("Density  f(y)")+
theme_classic()+
scale_fill_discrete(labels=c("Female", "Male"))+
geom_vline(xintercept =mean(agedf$edad), linetype="dotted", color="red")+
annotate("text", x=32, y=.005, size=2,label="Mean patient age")+ theme(legend.title=element_blank())

ggplotly(dp)
```

```{r, include= FALSE}

##**Ejemplo 2**  Distribución de edades para defunción por covid en México. Funciones estudiadas (Verbos tidyverse aplicados): Filter, select, mutate, summarise. ggplot 


open<-readRDS("open.rds")

def<-filter(open, resultado==1)%>%
  filter(fecha_def!="9999-99-99")%>%
  select(sexo, edad)
def$sexo[def$sexo == "1"] <- "Female"
def$sexo[def$sexo == "2"] <- "Male"  
def<-mutate(def,sexo=as.factor(sexo))

fmean<-filter(def, sexo=="Female")%>%
  mutate(varmean=mean(edad))%>%
   summarise(mean=mean(varmean))

mmean<-filter(def, sexo=="Male")%>%
  mutate(varmean=mean(edad))%>%
   summarise(varmean=mean(varmean))

  
p<-ggplot(def, aes(x=edad, fill=sexo))+
geom_density(alpha=.2)+  
labs(color="Sex")+
xlab("Person age  (years)")+
ylab("Density  f(y)")+
theme_classic()+
scale_fill_manual(values = c("brown","blue4"))+
geom_vline(xintercept = fmean$mean, linetype="dotted", color="purple")+
geom_vline(xintercept = mmean$varmean, linetype="dotted", color="blue")+
annotate("text", x=44, y=.005, size=3,label="Mean person age")+ theme(legend.title=element_blank())


ggplotly(p)
```





```{r,  fig.height = 8, fig.width = 12,  message=FALSE, warning=FALSE, include=FALSE}

####  Caso Aguascalientes

#**Ejemplo  3** Función de densidad para dos categorias. Funciones estudiadas: group_by, rename, left_join, filter, select, mutate, summarise. ggplot 



sdensity<-readRDS("density.rds")%>%
  mutate(s=as.factor(s))%>%
  filter(ENT_RESID!=33& ENT_RESID!=34& ENT_RESID!=35& ENT_RESID!=99)%>%
  group_by(ENT_RESID)

cat_e<-read.csv("cat_entidad.csv")%>%
select(-X)%>%
rename(ent_res=X.U.FEFF.EDO)%>%
mutate(ent_res=sprintf("%02d",ent_res))%>%
mutate(ENT_RESID=as.factor(ent_res))%>%
select(-ent_res)

sdensity<-left_join(sdensity,cat_e)



ags<-filter(sdensity,ENT_RESID=="01")%>%
  mutate(s=as.factor(s))


aplt<-ggplot(ags, aes(x=age,linetype=s, color=s))+
geom_line(stat="density")+
labs(linetype="Causa")+
xlab("Edad de la persona (Años)")+
ylab("Densidad f(y)")+
annotate("segment", x=29, xend=13, y=0.011, yend=0.011,  size=.3, arrow=arrow(length=unit(.2,"cm")))+
annotate("text", x=23, y=0.013, label="Alta incidencia")+
annotate("text", x=21, y=0.009, label="13<Edad<29")+
annotate("rect", xmin=13, xmax=29, ymin=0, ymax=0.03, alpha=.1,fill="black")+
scale_linetype(labels=c("Otras causas","Suicidio"))+
theme_classic()+
scale_color_manual(values = c("mediumturquoise","magenta1"))+
theme(legend.position="none")


aplt

```




```{r,  fig.height = 8, fig.width = 12, message=FALSE, warning=FALSE, include=FALSE}

#**Ejemplo 4** Función de densidad multiples bases de datos. (32 estados de México). Herramienta de análisis: stat=density 

#Funciones estudiadas: **facet_wrap**
mxplt<-ggplot(sdensity, aes(x=age,linetype=s, color=s))+
geom_line(stat="density")+
labs(linetype="Causa")+
xlab("Edad de la persona (Años)")+
ylab("Densidad f(y)")+
facet_wrap(~DESCRIP, nrow=6)+
scale_linetype(labels=c("Otras causas","Suicidio"))+
  theme_classic()+
scale_color_manual(values = c("mediumturquoise","magenta1"))+
  theme(legend.position="none")

mxplt
```



**Nota sobre ventajas de las KDF: **

Nos permiten distinguir rapidamente características como los atributos de tendencia central, la presencia de  bimodalidad,  sesgo), también facilitan la comparación rápida entre diferentes conjuntos de datos.

Por ejemplo vemos en la figura siguiente que la **media** de un conjunto de datos con **sesgo positivo** se ubica **a la derecha** de la mediana.

**IMPORTANTE ** En este caso **la mediana** será la medida de tendencia central **preferible** o que describe con mayor precisión la ubicación del **dato central**, por que divide la distribución de la curva en dos areas iguales. 

![](/img/asimetria_positiva.jpg)

**Una propiedad importante:**

*"El área por debajo de la curva, y entre cualquier intervalo de valores, es la proporción de todas las observaciones que están situadas en dicho intervalo."*

+ Note que el area por debajo de la KDF, siempre es igual a 1. 

En otras palabras, si sumamos la proporción que integra la distribución total, tenemos el 100% 


###  Función de distribución de probabilidad PDF.

La curva que describe la probabilidad para cada valor, (la distribución de probabilidad),  es una **función contínua** y se denomina función de **densidad de probabilidad.** $p(x)=$**(PDF).** 


La **PDF** (función de densidad de una variable **aleatoria continua**), es una función que describe la probabilidad de que una variable aleatoria **X** tome un valor particular **x**  

**X** Variable aleatoria ej. ingreso
**x:** valor particular de la variable aleatoria. ej. $60 000.

La pregunta es ¿cuál es la probabilidad de que la variable **X** tome un valor particular **x**

La **PDF** tiene las propiedades: 


> $${p(x)}\geq0, 	\forall x \in R$$
> $$\int_{-\infty}^{\infty}     p{(x)}dx=1$$

![](/img/PDF_probability.jpg)

La integral de p(x) en el rango [a,b] representa la probabilidad de encontrar el valor **x** en ese **intervalo**.  El area bajo la curva, determina el valor de esa probabilidad en el intervalo particular. A esta area la denominamos función de densidad acumulada **CDF** 


## Ejercicio.

![](/img/ejerciciodensidad.jpg)


##### Aplicación de la **CDF** a la estimación de los **percentiles.**

La manera más simple de determinar los percentiles es mediante la estimación de la **función de densidad acumulada** **CDF** (por sus siglas en inglés). 
$$CDF(x)=\int_{-\infty}^x PDF(x)dx$$

#### ECDF Función de densidad acumnulativa empírica.

Conocer la frecuencia es importante pero muy a menudo necesitamos conocer que proporción de nuestras observaciones se ubica por debajo de un cierto umbral.

Para este propósito es muy útil estimar la frecuencia acumulada. 

Una representación contínua de este concepto es mediante la función empírica de **densidad  acumulada** 


Una nota sobre la apliación de la **CDF** en la obtención de los **Percentiles** 

Los percentiles permiten determinar el valor por debajo del cual, ocurre un determinado porcentaje de los datos.

La  función de distribución acumulada**CDF** es la integral de la **función de densidad de probabilidad (PDF)**. 

Define la probabilidad de observar un valor **x** en un rango determinado **[a,b].** 
Formalmente 

![](/img/cdf.jpg)


Revisemos ahora en la práctica como podemos estimar la **función de densidad**  usando **Python.**

**"Sesión_2_Tendencia central.ipynb"**

Nota sobre implementación en **Python**. Esta  nos permite observar el porcentaje acumulado de datos en el eje de las ordenadas al orígen y los valores observados en el eje de las absisas.

La pregunta básica que podemos responder estimando la **CDF**, es ¿qué porcentaje de los datos se ubica por debajo de cualquier umbral deseado?
