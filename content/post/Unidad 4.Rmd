---
title: "Unidad 4"
output: html_document
date: '2022-12-14'
---

```{r set-global-options, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE, 
                      cache = FALSE,
                      include = TRUE,
                      collapse = FALSE,
                      dependson = NULL,
                      engine = "R", # Chunks will always have R code, unless noted
                      error = FALSE)                     

```



```{r silent-packages, echo = FALSE, eval = TRUE, message=FALSE, include = FALSE}
library(ggrepel)
library(tidyverse)
library(scales)
library(ggpubr)
library(wooldridge)
```


## Regresión Lineal simple.


##### Objetivo de la sesión:

Presentar el modelo de Regresión lineal. Orígenes del concepto de regresión, características y estructura del modelo, expresión matemática. 


*All models are wrong, but some are useful. Box (1979)*

##### Metas de la sesión de hoy: 

+ Aprender a Modelar relaciones estadísticas (estocásticas) no deterministas. Mediante el modelo de regresión lineal.

+ Conocer la técnica de mínimos cuadrados ordinarios.


### Orígenes de la noción de regresión

La noción de Regresión lineal. **¿Qué entendemos por regresión?**

A pesar de las multiples contribuciones y desarrollos recientes sobre la aplicación de este concepto en el contexto del análisis estadístico el desarrollo de la técnica se debe al francés **Legendre en 1805** y su uso generalizado bajo la denominación del modelo de **regresión lineal** se atribuye al antropólogo británico **Francis Galton** (primo de **Charles Darwin**)  

De acuerdo con Stigler en su libro [Statistics on the table](https://drive.google.com/file/d/1NgCTeIg4Tclt-TsKpHV_wmXvk4TY7dZ9/view?usp=sharing) *"The story is an exciting one involving, science, experiment, mathematics, simulation and one of the great thougth experiments of all times."* (Stigler, 1999)

De acuerdo con **Galton** en su obra [Hereditary Genius 1869](https://drive.google.com/file/d/1Zd3ORNina7jKPXe176nhUNP3wRSElsMv/view?usp=sharing). La mezcla de características genéticas a lo largo de la descendencia explica el grado de talento observado en los individuos.

Su trabajo aborda entre otros elementos como el talento se manifiesta através de las generaciones. Ej. los Bachs, (Johann Sebastian Bach) los Bernoulli, (Daniel Bernoulli ), etc.

Sus observaciones parecian indicar que el talento sistemáticamente tendía a disminuir a medida que se consideraba a los miembros de la familia más distantes, tanto hacia "arriba"  en el árbol genealógico como hacia "abajo". La observación se manifiesta incluso al referirnos a otras especies. 

Galton: *"if a man breeds from strong,  well-shaped dogs, but of mixed pedigree, the puppies will be sometimes, but rarely, the equals of their parents. They will commonly be of a mongrel, nondescript type, because ancenstral peculiarities are apt to crop out in the offspring".*

#### Definiciones.

Regresión es el estudio de la distribucion condicional de $Y|x$ de la variable de respuesta **Y** dado un vector $pX1$ de variables predictivas. En el modelo de regresión lineal $$Y=\beta^T X+\epsilon$$ 

Definición preliminar sobre el concepto de regresión. 

$Y$ es condicionalmente dependiente de $x$ dada una combinación lineal $\beta^Tx$ de las variables predictivas.

Básicamente estamos modelando una **relación de dependencia**. En este caso la variable dependiente es **cuantitativa**, no categórica, si bien los regresores o variables **explicativas(independientes)** pueden  ser de tipo categórico o numérico, (considerando que se realiza la codificación apropiada).


El concepto matemático en el que se basa el modelo de regresión es una relación de dependencia entre una variable denominada dependiente denotada generalment por**Y**, y un conjunto de variables explicativas o independientes, denotadas por el vector X  que incluye las variabkes explicativas individuales $x_i$.


En términos matemáticos la siguiente ecuación expresa la forma básica del modelo $$Y=a+bX+e$$

Note que esta ecuación representa una recta.


**Recomendación** [Linear regression Analysis.Xin Yan, 2009](https://drive.google.com/file/d/1peEQsZpHYhGscI6PGUQJ5gCFrM59sT35/view?usp=sharing)

El proceso de modelar la relación entre la variable dependiente y las explicativas incluye la determinación de la **significancia estadística** esto es, el grado de confianza sobre que tanto la forma funcional utilizada reproduce la verdadera forma funcional que subyace a los datos.

El análisis de regresión es un proceso que permite **predecir** los valores de la variable de respuesta con base en los valores que toman las variables explicativas o indepedientes.

Este proceso se realiza mediante la estimación de los **parámetros** de la función lineal seleccionada y  a partir del criterio estadística de referencia. En el particular, OLE.  

Es una **técnica de estimación paramétrica**. En contraste regresiones de tipo no parametrico donde el objetivo no es estimar los parámetros de la función que describe la relación entre las variables del modelo.


## Característcas y utilidad de los modelos de regresión.

Uno de los rasgos de interés de la técnica de **regresión** desde la perspectiva de su aplicación es su capacidad para hacer **predicción** y su uso para hacer inferencia, por ejemplo cuando el objetivo es determinar  qué variables se relacionan con la variable de respuesta de interés o qué relación existe entre la variable dependiente y cada una de las variables explicativas. 

Desde la determinación de indicadores en el contexto del análisis de política públíca en áreas como sociología, salud, economía o bien cuestiones como  mortalidad por diabetes, migración y mercados laborales, el efecto de los gastos de camapaña sobre los resultados de votaciones, los efectos del presupuesto en educación sobre el desempeño educativo,etc,.

Para responder las preguntas de investigación de interés la formulación de un modelo (expresión  matématica de las relaciones entre las variables que nos interesan) es útil.

El énfasis en los modelos presentados en nuestro curso es en **datos no experimentales**,(Un tipo de datos común en ciencias sociales). Datos que no se generan mediante experimentos contralados sobre individuos, este ultimo tipo común en otras disciplinas como Biología, ciencias de la salud con los ensayos clínicos por ejemplo. 

##  Modelos y relación funcional entre variables.

Una forma de representar la relación entre dos variables es a través de un diagrama de dispersión. La siguiente gráfica presenta la realación entre dos variables años de educación y salario para una muestra de 526 individuos.


```{r, echo=FALSE,fig.width=8, fig.height=6, warning=FALSE, message = FALSE}
theme_set(theme_light())

data(wage1)
str(wage1)

ggplot(wage1,aes(educ,wage,colour = educ))+
  geom_point()+
  labs(title="Salario v.s Educación", x="Años de educación", y="Salario por hora (USD)")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ 
  scale_x_continuous(breaks = c(0, 6, 12,18))+
  labs(colour="Años")
```


A continuación se muestran posibles variables explicativas respecto a la variable dependiente salario, utilizando un scatter plot y una linea de tendencia generada mediante un modelo de regresión lineal. 


```{r, echo=FALSE, fig.width=10, fig.height=4, warning=FALSE, message = FALSE}

library(gridExtra)
data(wage1)

p1<-ggplot(wage1,aes(educ,wage))+
  geom_point(shape=1,color="blue")+
  labs(y="Salario por Hora (USD)", title="Años de Educación")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ 
  scale_x_continuous(breaks = c(0, 6, 12,18))+
  stat_smooth(method=lm, se=FALSE, colour="red")


p2<-ggplot(wage1,aes(exper,wage))+
  geom_point(shape=1,color="blue")+
  labs(title="Años de experiencia")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  theme(axis.title.y=element_blank())+
  stat_smooth(method=lm, se=FALSE, colour="red")



p3<-ggplot(wage1,aes(tenure,wage))+
  geom_point(shape=1,color="blue")+
  labs(title="Años con el empleador")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  theme(axis.title.y=element_blank())+
  stat_smooth(method=lm, se=FALSE, colour="red")


grid.arrange(p1, p2, p3, nrow = 1)
```

Note que  para cada variable se observa una  de posible relación positiva de distinta magnitud. 

El siguiente ejemplo  presenta la relación de tres posibles variables explicativas para una variable dependiente denominada ventas. 

La gráfica muestra la relación entre la variable dependiente ventas (expresada en miles) y los gastos en publicidad en tres diferentes medios  (variables independientes, expresada en miles de USD). n=200.

![](/img/r1.jpg)




Cada linea azul representa un modelo que puede usarse para predecir la variable **dependiente** **ventas** usando el procedimiento de mínimos cuadrados  (ordinary least squres **OLS**).

En términos  de notación la relación previa puede generalizarse como: 

$$Y=f(X)+\epsilon$$
Aquí $\epsilon$ representa un error aleatorio y $f()$ representa la forma funcional  que caracteriza la relación entre la variable dependiente y las variables explicativas.  Esta forma funcional se **asume lineal** en el modelo de regresión que estudiamos en esta sesión. 

Consideremos el siguiente ejemplo sobre el mercado laboral en el que se trata de analizar el efecto que determinantes como la educación, la experiencia o el nivel de entrenamiento tienen sobre la productividad.

En términos teóricos se espera que estos determinantes tengan un impacto sobre la productividad y considerado la relación positiva entre la productividad y el salario, se prevé un impacto en este último.

$$y=f(x_1, x_2, x_3)$$
$$ salario=f(educ, exper, training)$$

Donde: 

Salario: remuneración por hora.

educ: Años de educación formal.

exper: Años de experiencia en el trabajo. 

training: semanas dedicadas a entrenamiento.


Note el tipo de variables utilizadas, en este caso son numéricas (**cuantitativas** continuas), si bien en este curso el énfasis está en el uso de variables cuantitativas, es posible tambien incluir variables categóricas (*factors* que registran información **cualitativa** como sexo, la pertenencia a una región, etc.)

$$salario=\beta_0+\beta_1 educ +\beta_2 exper +\beta_3 training +\epsilon $$

![](/img/lm1.jpg)

**Note** en este caso que la relación verdadera entre las variables aunque se asume lineal puede tener otra forma funcional. Este aspecto es importante por que la precisión de las estimaciones realizadas depende de la selección de la forma funcional adecuada. 

**Note** La presencia de observaciones por arriba y algunas por abajo de la linea que expresa la relación funcional. Esto implica la presencia de errores respecto de medición entre la forma funcional y el valor real de los datos (estos errores son expresión de la naturaleza estocástica de las observaciones en la muestra). 

En términos formales la variación que observamos en la gráfica se puede expresar como: 

![](/img/lm2.jpg) 

La **precisión** en las predicciones obtenidas mediante un modelo, es decir la variación entre los valores estimados $\hat{Y} =f(X)$ y el valor real de la variable $Y$ se integra fundamentalmente por dos partes: 

La variación **reductible**, que se genera por la diferencia entre la forma funcional real y la forma funcional estimada por el modelo.

Y la variación **irreductible** atribuida a la naturaleza estocástica de las variables estudiadas que se ven influenciadas por un número infinito de factores mismos que no se capturan por el modelo. 


Note la importancia de la herramienta gráfica como  un paso preliminar a la construcción del modelo y en ayuda para conocer la relación posible entre las variables explicativas usadas como regresores y la variable dependiente que deseamos predecir o hacer inferencia.

Como primer paso una herramienta gráfica fundamental del análisis de regresión es la representación de los datos mediante un **scatter plot** bidimensional.

En algunos casos la relación entre la variable dependiente y las variables explicativas se basa en una teoría establecida con un grado elevado de concenso y precisión, no obstante en otros casos la relación entre variables se puede inferir a partir de su comportamiento, en cualquier caso las gráficas de dispersión son de suma importancia para explorar la relación entre las variables del modelo propuesto. 

Lectura **Recomendada** [Sanford Weisberg ,2010](https://drive.google.com/file/d/1pFd7mqYVSZ2qgNNIdZGUw51O1Cp5Y81P/view?usp=sharing)


## Sobre el proceso de modelado


El primer paso es seleccionar al forma funcional que vincula a la variable dependiente  con las explicativas. En esta caso el modelo que estudiaremos primero es cuando $f$ es lineal y bajo la estimación paramétrica.  

$$f(X)=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ...+\beta_p X_p $$

Este es un modelo lineal con $p+1$ parámetros a estimar, (uno por cada variable explicativa y el intercepto.)

El objetivo es estimar los parámetros con base en la forma funcional tal que se aproxime lo mas posible a la forma funcional real. Para lo cual un **método paramétrico clásico** es el de **minimos cuadrados** (OLS).

$$Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ...+\beta_p X_p $$
Una desventaja potencial del este **método paramétrico** es que la pre-definición  de la forma funcional limita la precisión en comparación a estimaciones con métodos no paramétricos. 

La siguiente representación gráfica corresponde al caso de la forma funcional lineal estimada con métodos paramétricos mediante  MCO a), b) la forma funcional real y la estimación de un modelo no-parámetrico en c).  



![](/img/lm3.jpg)

La desventaja en métodos no-parámetricos es que su misma precisión implica cierta rigidez para su replica en muestras distintas o con observaciones nuevas.

El punto en la selección del modelo empleado para  representar la forma funcional es el **balance entre interpretabilidad y precisión** (entendiendo la precisión respecto a la flexibilidad para reproducir diversas formas funcionales no solo formas lineales como es el caso del método de RL por MCO.).

De hecho el modelo de regresión lineal es conveniente cuando buscamos **hacer inferencias**   dada su relativa facilidad de interpretación. 

En síntesis es importante tener en mente que hay un **trade off** entre el nivel de precisión en los estimados que pueden generarse por el modelo (el grado de sesgo *Bias* ej. Qué tanto nuestra forma funcional se aproxima a la forma funcional real que subyace a los datos) y la varianza, entendida esta como  la magnitud en la que $\hat{Y}$ cambia con la estimación de otras muestras o bases de datos.  


### Preguntas relevantes en las que el análisis de regresión lineal puede ser  útil.

Sea $$y = \beta_0 + \beta_1 x +\epsilon$$ un modelo lineal.


1.-Exsiste una relación entre las variables propuestas. Ej. salario y años de educación, peso del recien nacido y fumar durante el embarazo, ventas y gastos en publicidad para diferentes medios, flujos migratorios y características de los estados (USA), etc,.?

En este sentido nuestra meta es **proporcionar evidencia** de la existencia de una relación entre variable dependiente y explicativas.


2.- ¿Qué tan fuerte es la relación, entre las variables estudiadas. ¿Cuál es la magnitud de este grado de asociación en caso de existir?  

Nos permite esta asociación hacer predicciones  con un alto grado de exactitud? O la predicción es solo ligeramente mejor que una hecha de manera aleatoria.

3.- ¿De entre un conjunto de posibles variables explicativas ¿cuál  variable contribuye con mayor peso para el comportamiento de la  variable dependiente.

Para responder esta pregunta necesitamos determinar de los efectos individuales para cada variable.

4.¿Qué tan preciso es nuestro estimado del efecto individual que las variables explicativas tienen sobre la variable dependiente? Para un incremento de una unidad en las variables independientes ¿cuál es el efecto en al variale dependiente.

Suponiendo.

$$\Delta Y=\beta_1 \Delta x,  si \Delta \epsilon= 0$$
Donde $\beta_1$ es el parámetro que indica la **pendiente** en la relación entre y, x.



Note que $\Delta \epsilon=0$ es un supuesto que refleja  el hecho de que los otros factores, que de hecho no se miden, son constantes. 

5.-  ¿Cuál es el nivel de precisión en la predicción de la variable dependiente?

Para un nivel dado de las variables independientes ¿cuál es nuestra predicción de la variable dependiente?


6.- ¿Es la relación entre las variables lineal? Tal vez una combinación en las variables independientes tiene un mayor efecto en la variable dependiente que el incremento por separado en  cada una de las variables. 

Ej. La experiencia puede impactar las habilidades de un año a otro en forma creciente o tener un mayor imapcto considerando una combinación de educación y experiencia.  Este aspecto implica la estimación de *interacciones entre las variables*



Leer  Capítulo 3. [An Introduction to Statistical Learning](https://drive.google.com/file/d/1zaKc1XWvELsUgZrS32QdpHoM4-w0y7Qx/view?usp=sharing).


#### Ejemplo

Buscamos predecir el valor medio de las viviendas (Y) con base en 12 variables independientes o regresores (x). La base de datos cuenta con 560 observaciones correspondientes a información en una ciudad.

```{r}
install.packages("ISLR2")

library(ISLR2)
attach (Boston)

#  Data for 506 census tracts in Boston.
# medv (median house value)
# lstat (percent of households with low socioeconomic status)

# rm (average number of rooms per house),

# age (average age of houses)


m.fit <- lm(medv ∼ lstat , data = Boston)

summary(lm.fit)

plot (lstat , medv)
abline (lm.fit)

```

 **Note el orden en la inclusión de los argumentos** primero tenemos la variable dependiente, seguida por las variables independientes.
 
 
#### Formalización del modelo

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(wooldridge)
library(ISLR)
```



En términos matemáticos  podemos describir la relación entre las variables del modelo con la expresión siguiente:

\begin{align}
Y \approx \beta_0 + \beta_1 X
(\#eq:align)
\end{align}

Una vez que hemos estimado los parámetros del modelo podemos usarlos para hacer la predicción de $\hat y$ 


\begin{align}
\hat y= \hat \beta_0 + \hat \beta_1 x
(\#eq:mod)
\end{align}

![](/img/lm4.jpg)


Note que la diferencia entre la linea de regresión y cada observación (lineas grises) da cuenta del error de predicción.

$$e_i=y_i - \hat y_i$$
El modelo de regresión toma la suma al cuadrado de estos errores para determinar la  estimación de la linea de mejor ajuste.

Suponemos una serie de axiomas que conforman el fundamento teórico de esta modelo a partir del **Teorema de Gasuss Markov**. 

Suponemos un valor esperado para el término aleatorio (*random noise*) $E(\epsilon)=0$. Esto es, suponemos que la distribución de estos factores no observados, en promedio tienen media cero.


#### Supuesto de Media condicional cero.

Sean $x$ y $\epsilon$ variables  aleatorias. 

Suponemos que el valor promedio de las características no observadas no depende del valor particular que tome $x$  $$E(\epsilon|x)=E(\epsilon)=0$$

El valor esperado de las características no observadas, es independiente de $x$  y en términos promedio es cero. 

Definición: 

*Residual Sum of Squares* **RSS**.


$$RSS=e_1^2 +e_2^2+...+e_n^2$$
\begin{align}
RSS= (y_1- \hat \beta_0 - \hat \beta_1 x_1)^2 + (y_2- \hat \beta_0 - \hat \beta_1 x_2)^2+...+(y_n- \hat \beta_0 - \hat \beta_1 x_n)^2
(\#eq:RSS)
\end{align}

![](/img/rss1.jpg)
El enfoque de Mínimos Cuadrados Ordinarios **MCO** (Ordinary Least Squares OLS por sus siglas en inglés), selecciona los parámetros $\hat \beta_0 , \hat \beta_1$ que minimizan la **RSS**. 

#### Coeficientes acorde con el criterio MCO.


\begin{align}
\hat \beta_1 =\frac{ \sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n} (x_i-\bar{x})^2},
(\#eq:beta)
\end{align}


El parámetro $\hat \beta_1$ es el cociente  de la covarianza  de $(x,y)$ y la varianza de $x$.  Dividir por $\frac{1}{n}$ ambos factores no hace diferencia alguna.     

$$\hat \beta_0 =\bar y- \hat \beta_1 \bar{x}$$
Donde $$\bar{y}\equiv\frac{1}{n} \sum_{i=1}^{n} y_i$$ y  $$\bar{x} \equiv \frac{1}{n} \sum_{i=1}^{n} x_i$$

Note por la definición  que si $x$ e $y$ tienen correlación positiva, $\Rightarrow \hat \beta_1>0$ y si $\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})<0 \Rightarrow \hat \beta_1<0$
dado que $\sigma^2$ tiene un min val=0, ie. $\sigma^2$ nunca es negativo$

Considere la siguiente representación del modelo lineal a partir del valor esperado y con los supuestos $E(\epsilon)=0$ e independencia entre la parte aleatoria del modelo  y las variables explicativas:  y  $cov(x,\epsilon)=E(x \epsilon)=0$

\begin{align}
E(y-\beta_0-\beta_1 x)=0
(\#eq:ve)
\end{align}



\begin{align}
E [x(y-\beta_0-\beta_1 x)]=0
(\#eq:ve1)
\end{align}


Los parámetros muestrales correspondientes

\begin{align}
\frac{1}{n}\sum_{i=1}^{n} (y_i-\hat \beta_0- \hat \beta_1 x_i) =0
(\#eq:ve3)
\end{align}


\begin{align}
\frac{1}{n} \sum_{i=1}^{n} x_i(y_i-\hat \beta_0- \hat \beta_1 x_i)=0
(\#eq:ve4)
\end{align}

Ecuaciones \@ref(eq:ve3) y \@ref(eq:ve4) son las condiciones de primer orden a resolver acorde con los principios de optimización.

A partir de las propiedades de suma estas ecuaciones se pueden reescribir:

\begin{align}
\bar{y}=\hat \beta_0- \hat \beta_1 \bar{x} 
(\#eq:ve5)
\end{align}

Reescribiendo $\hat \beta_0$  y  $\hat \beta_1$  en términos de $\bar{y}$ y $\bar{x}$ tenemos: 

\begin{align}
\hat \beta_0= \bar{y} - \hat \beta_1 \bar {x}=0
(\#eq:ve6)
\end{align}


Sustituyendo \@ref(eq:ve6) en \@ref(eq:ve4) tenemos

$$\sum_{i=1}^{n} x_i [ y_i-(\bar y- \beta_1 \bar{x})- \hat \beta_1 x_i]=0$$




Reordenando tenemos: 

$$\sum_{i=1}^{n}x_i(y_i-\bar{y})= \hat \beta_1  \sum_{i=1}^{n} x_i(x_i-\bar {x})$$ 


Considerando propiedades de suma tenemos:

$$\sum_{i=1}^{n} x_i(y_i- \bar{y})= \sum_{i=1}^{n} (x_i- \bar{x})(y_i- \bar{y}) , \sum_{i=1}^{n} x_i(x_i- \bar{x})=\sum_{i=1}^{n} (x_i- \bar{x})^2$$
Si $\sum_{i=1}^{n} (x_i- \bar{x})^2>0$ lo cual se cumple cuando **NO** todos los valores de la muestra son iguales, entonces:


\begin{align}
\hat \beta_1=  \frac{\sum_{i=1}^{n} (x_i- \bar{x})(y_i- \bar{y}) }{\sum_{i=1}^{n} (x_i- \bar{x})^2}
(\#eq:ve7)
\end{align}




Considerando el **supuesto de Media condicional cero** $E(\epsilon|x)=0$  y representando el valor esperado de \@ref(eq:mod) condicional en $x$ tenemos una función lineal de $x$: 

\begin{align}
E(y|x)= \beta_0+ \beta_1 X 
(\#eq:expval)
\end{align} 

La siguiente gráfica muestra esta idea para la función lineal  $\beta_0+\beta_1 x$ con la distribución de los valores de $y$ centrados en  $E(y|x)$

![](/img/lm6.jpg){width=400px}

Lo que significa que **un incremento de una unidad en $x$ cambia el valor esperado de $y$ por el monto denotado $\hat \beta_1$**, para cualquier valor dado de $x$, $y$ que está centrado alrededor de $E(y|x)$. 

En términos de la ecuación lineal el estimado de la pendiente es: 
$$\hat \beta_1= \frac{\Delta \hat y}{\Delta  x}$$ 
$$\Delta \hat y=\hat \beta_1 \Delta  x$$

En la siguiente figura se presenta ésta idea con una muestra teórica de valores aleatorios para $x$ y $y$.  

El panel a) muestra en linea roja la verdadera relación en entre $x$ y $y$, la linea azul es la linea de regresión obtenidad por MCO que se basa en los datos observados (scatter points). 

En panel b) se incluyen las lineas de regresión para  10 modelos con muestras aleatorias en la población de $x$, vemos que cada modelo permite generar una línea distinta si bien,  en *promedio* estas siguen la trayectoria de la linea de regresión poblacional (roja).

En el panel c) Observamos el efecto del criterio de minimos cuadrados  con la obtensión del *RSS* de menor valor para los parámetros $\hat  \beta_0$  y $\hat \beta_1$


![](/img/lm5.jpg)

**Ejemplo 1**

Considerando el ejemplo  de la relación entre salario y años de educación $log(wage)=\beta_0 +\beta_1 educ+ \epsilon$  

Con el logaritmo de salario (**wage** en dólares por hora) para medir el cambio porcentual ante el incremento de un año adicional de educación  con la variable **educ** que se registra en años.

Recordamos que  $\epsilon$ integra un conjunto de variables que se asumen constantes como experiencia, habilidades, etc., Y estas tienen una distribución representada por el valor esperado normalizado en cero $E(\epsilon)=0$. (algunas personas tienen más habilidad natural que otras, más o menos experiencia, considerar el supuesto de la distribución con media cero es consistente con este hecho empírico).

La gráfica siguiente muestra la linea de regresión (rojo) para las variables salarios y educación con las observaciones (azul) como un scatter plot. 


```{r, , echo=FALSE}
data(wage1)
theme_set(theme_light())

ggplot(wage1,aes(educ,lwage))+
geom_point(shape=1,color="blue")+
stat_smooth(method=lm, se=FALSE, colour="red", size=.2)+
labs(title="Log Salario v.s Educación", x="Años de educación", y="Salario por hora (USD)")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ 
  scale_x_continuous(breaks = c(0, 6, 12,18))
```


Notamos una relación positiva y a medida que la persona tiene más años de educación en promedio percibe un mayor salario.

Tenemos una  linea de regresión obtenida bajo el criterio de MCO y observamos una distancia entre la linea de regresión y las observaciones, sin embargo sabemos que la linea de regresión minimiza esa distancia.


Considerando los parámetros estimados para este ejemplo sobre la relación salario y años de educación tenemos que $\hat \beta_0=0.583773$  y $\hat \beta_1=0.082744$

Lo que implica que un año adicional de educación se relaciona con un incremento del 8.2% en el salario por hora percibido.


**Nota** sobre el uso de logaritmos en la forma funcional del modelo de regresión lineal.

![](/img/logs.jpg)

```{r, echo=FALSE}
data(wage1)

log_wage_model <- lm(lwage ~ educ, data = wage1)
summary(log_wage_model)
```


#### Ejemplo 2

Consideremos ahora la base de datos: *Height and weight of school children* de Lewis, T., & Taylor, L.R. (1967) en el estudio introduction to Experimental Ecology, que contiene 236 observaciones para 5 variables en una muestra de niños en edades entre 11 A 17 años, para estudiar la relación  entre dos variables la altura y la edad. En términos teóricos se ha demostrado que entre la población munidal  se tiene una estrecha relación entre estas variables. 


```{r,, echo=FALSE }


heightweight<-readRDS("heightweight.rds")

theme_set(theme_light())

heightweight%>%
  mutate(sex=ifelse(sex=="f","Mujer","Hombre"))%>%
  ggplot(aes(x=ageYear, y=heightIn, colour=sex)) +
geom_point(alpha=.5) +
scale_colour_brewer(palette="Set1")+
geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  labs(x="Edad en años", y="Estatura (pulgadas)"  )+
  labs(colour="Sexo")
```  

```{r, echo=FALSE}

heightweight<-readRDS("heightweight.rds")

lm( heightIn~ ageYear, data = heightweight)%>%
summary()

M<-heightweight%>%
  filter(sex=="f")

lm( heightIn~ ageYear, data = M)%>%
summary()

H<-heightweight%>%
  filter(sex=="m")

lm( heightIn~ ageYear, data = H)%>%
summary()
```

Notamos  un efecto positivo, que es de mayor magnitud para los niños. Con un incremento de $\hat \beta_1= 2.03$ pulgadas por un año adicional. Mientras el incremento para el sexo femenino es ligeramente inferior. $\hat \beta_1=1.2$

Note además que el modelo se estima con las varaibles en niveles por lo que la interpretación se hace considerando las unidades de cada variable en este caso edad en años y estatura en pulgadas.

#### Ejemplo 3

Considermos ahora una situación en la que la variable dependiente es ventas (monto expresado en unidad monetaria ej. miles de dólares). Y las variables independientes corresponden a gastos en publicidad para tres medios:  t.v.,  newspapers,  radio. La muestra contiene 200 observaciones para las variables: Y=ventas,  x=gasto en publicidad dirigido a tv. y corresponden a 200 puntos de venta.

El modelo de regresión siguiente muestra la relación entre la variable ventas y los gastos en publicidad para el medio t.v. (gasto expresado en miles de USD y y ventas en unidades). 



```{r, echo=FALSE}

ad<-read.csv("Advertising.csv")

 p<- ggplot(ad,aes(x=TV, y=sales)) +
geom_point(alpha=.5) +
scale_colour_brewer(palette="Set1")+
geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  labs(x="Gastos en TV (Miles USD)", y="Ventas (Miles unidades)"  )+
  labs(colour="Sexo")
p

lm( sales~ TV, data = ad)%>%
summary()

```

Los parámetros estimados del modelo indican que un incremento de una unidad adicional  (en este caso  un incremento de $\$ 1000$) en la variable gasto en publicidad en TV se asocia con un incremento de 47 unidades en la variable ventas $\hat \beta_1=0.047537$


#### Ejemplo 4

Consideremos ahora la relación entre el Sueldo que perciben los CEO's (Chief Executive Officer) y el indicador de desempeño financiero *return on equity* **roe** este es un indicador clásico que es utilizado como *proxy* para medir el desempeño de un CEO. La muestra contiene 209 observaciones y  12 variables con información financiera publicadas por un estudio de *Businessweek* de Mayo 6, 1991. 

La variable dependiente es el Sueldo (expresado en miles USD). En términos teóricos se supone que existe una relación positiva entre el Sueldo que un CEO percibe e indicadores de desempeño de la compañia, en este sentido el    **roe** es una variable que recoge el desempeño financiero (variable que se encuentra expresada en porcentaje).

Estimaremos ahora el modelo de regresión lineal $$y=\hat \beta_0 + \hat \beta_1 x+\epsilon$$  $$salario=\beta_0 + \hat \beta_1 roe+\epsilon$$  para conocer si la hipótesis sobre la relación entre las variables se cumple y en su caso la magnitud de los parámetros $\hat \beta_0, \hat \beta_1$

```{r, echo=FALSE}

library(wooldridge)
data(ceosal1)
str(ceosal1)

summary(ceosal1$salary)
summary(ceosal1$roe)

lm( salary~ roe, data = ceosal1)%>%
summary()

```
Como es costrumbre en primera instancia exploramos la estructura de los datos. Tenemos 209 observaciones, 12 variables.  

En segundo lugar analizamos las estadísticas descriptivas. 
Para la variable Sueldo tenemos  una media de $\$1,281,000$ usd anuales, un mínimo de $\$223,000$ y un máximo de $\$14,822,000$. El rendimiento del capital promedio es $17.18\%$, máximo de $56.30\%$ y mínimo de $.50\%$

En tercer lugar note por el  parámetro $\hat \beta_1$ que hay una relación positiva ente Sueldo del CEO y el roe de la compañia, con $\hat \beta_0=963.19$ lo que implica que si el ROE es cero, la predicción del sueldo es $\$963,190$ y la pendiente   $\hat \beta_1=18.50$. Así $\Delta x=1\%\Rightarrow \Delta y=\$ 18,500$ En otras palabras ante **un incremento de 1% en el roe**, en promedio, el sueldo del CEO $i$ cambia  en  $\$ 18,500$  usd anuales.

La siguiente gráfica muestra estas relaciones distinguiendo por dos escenarios de interés  (con variables categóricas)  

```{r, echo=FALSE}

library(wooldridge)
data(ceosal1)
theme_set(theme_light())
summary(ceosal1$salary)
csa2<-ceosal1%>%
mutate(ventas=as.factor(ifelse(sales>7177,"High","Low")),
consprod=as.factor(ifelse(consprod==1,"Consumo","No")))


ggplot(csa2,aes(roe,salary,color=ventas))+
  geom_point(shape=1,color="darkgreen")+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  labs(x="Rendimiento sobre capital (%)", y="Sueldo (Miles USD)" )

ggplot(csa2,aes(roe,salary,color=consprod))+
  geom_point(shape=1,color="darkgreen")+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  scale_colour_brewer(palette="Set1")+
  labs(color="Empresa")+
  labs(x="Rendimiento sobre capital (%)", y="Sueldo (Miles USD)" )
  

```

## Actividad 3


#### Ejercicio 1 

Considere el siguiente escenario sobre procesos electorales. Sea una [muestra](https://drive.google.com/file/d/1lzbPJYbvk2C7uRk8JMB0RT3tvpoTl4X8/view?usp=sharing) de 173 resultados de votaciones distritales en un proceso entre dos partidos. La variable dependiente es el porcentaje de voto que recibió el candidato en la elección y la variable independiente es la proporción del gasto total de campaña correspondiente al candidato. Estudio de  M. Barone and G. Ujifusa, The Almanac of American Politics, 1992.


a) Estime el modelo de regresión y determine $\hat \beta_0, \hat \beta_1$.

b) ¿Cuál es el efecto en el porcentaje del voto ante un incremento de $1\%$ en los gastos de campaña del candidato. 

c) Estime el porcentaje del voto recibido si la proporción en gasto de campaña es de $62\%$   $\Delta \hat voto= \hat \beta_1  \Delta gasto$


```{r, eval=FALSE, echo=FALSE}
theme_set(theme_light())


vote1<-readRDS("vote1.rds")
data(vote1)
str(vote1)

lm(voteA~shareA,data=vote1)%>%
summary()

351.687/(351.687+50.532)
```

```{r,echo=FALSE}
vote1<-readRDS("vote1.rds")%>%
ggplot(aes(shareA,voteA))+
  geom_point(shape=1)+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  labs(x="Gastos en campaña (%)", y="Voto (%)")
vote1
```



#### Actividad para participación en clase

¿Cómo interpretamos una modelo de regresión cuya variable dependiente se expresa mediante una transformación logarítmica? 

¿Qué interpretación tiene el coeficiente $\hat \beta_1$ en el modelo en logarítmos para las variables dependiente y explicativa. 


#### Recomendación de bibliografía de estudio en casa para el tema.

Leer sección *The Effects of Changing Units of Measurement on OLS Statistics*  y sección: *Incorporating Nonlinearities in Simple Regression* en Wooldridge (pags. 40-44)


### Términos clave

+ Coeficiente de la pendiente $\hat \beta_1$

+ coeficiente de intercepto $\hat \beta_0$

 





### Términos clave 

**Least squares:** Noción atribuida a Gauss. Criterio para la estimación del modelo de regresión lineal.

### Referencias


Box, G. E. P. (1979). Robustness in the strategy of scientific model building. In R. Launer & G. Wilkinson (Eds.), Robustness in statistics (pp. 201–235). New York, NY: Academic Press.
